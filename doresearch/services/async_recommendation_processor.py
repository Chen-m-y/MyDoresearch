"""
å¼‚æ­¥æ¨èè®¡ç®—å¤„ç†å™¨
è´Ÿè´£åå°AIæ¨èè®¡ç®—ã€ç¼“å­˜ç®¡ç†å’Œå¢é‡æ›´æ–°
"""
import json
import hashlib
import threading
import time
from datetime import datetime, timedelta
from typing import Dict, List, Optional
from models.database import Database
from services.ai_based_recommender import AIBasedRecommender
from config import DATABASE_PATH


class AsyncRecommendationProcessor:
    """å¼‚æ­¥æ¨èè®¡ç®—å¤„ç†å™¨"""
    
    def __init__(self):
        self.db = Database(DATABASE_PATH)
        self.ai_recommender = AIBasedRecommender()
        self.is_running = False
        self.processing_thread = None
        
        # é…ç½®å‚æ•°
        self.CACHE_EXPIRY_HOURS = 24  # ç¼“å­˜24å°æ—¶è¿‡æœŸ
        self.MAX_RETRIES = 3  # æœ€å¤§é‡è¯•æ¬¡æ•°
        self.PROCESS_INTERVAL = 60  # ä»»åŠ¡å¤„ç†é—´éš”ï¼ˆç§’ï¼‰
        
    def start_background_processor(self):
        """å¯åŠ¨åå°å¤„ç†çº¿ç¨‹"""
        if not self.is_running:
            self.is_running = True
            self.processing_thread = threading.Thread(target=self._process_loop, daemon=True)
            self.processing_thread.start()
            print("âœ… å¼‚æ­¥æ¨èå¤„ç†å™¨å·²å¯åŠ¨")
    
    def stop_background_processor(self):
        """åœæ­¢åå°å¤„ç†"""
        self.is_running = False
        if self.processing_thread:
            self.processing_thread.join(timeout=5)
        print("âš ï¸ å¼‚æ­¥æ¨èå¤„ç†å™¨å·²åœæ­¢")
    
    def _process_loop(self):
        """åå°å¤„ç†å¾ªç¯"""
        while self.is_running:
            try:
                # å¤„ç†å¾…æ‰§è¡Œçš„ä»»åŠ¡
                self._process_pending_jobs()
                
                # æ¸…ç†è¿‡æœŸç¼“å­˜
                self._cleanup_expired_cache()
                
                # æ£€æŸ¥ç”¨æˆ·å…´è¶£å˜åŒ–ï¼Œè§¦å‘å¢é‡æ›´æ–°
                self._check_interest_changes()
                
                # ç­‰å¾…ä¸‹ä¸€æ¬¡å¤„ç†
                time.sleep(self.PROCESS_INTERVAL)
                
            except Exception as e:
                print(f"âŒ åå°å¤„ç†å¼‚å¸¸: {e}")
                time.sleep(self.PROCESS_INTERVAL)
    
    def _process_pending_jobs(self):
        """å¤„ç†å¾…æ‰§è¡Œçš„æ¨èä»»åŠ¡"""
        try:
            conn = self.db.get_connection()
            c = conn.cursor()
            
            # è·å–å¾…å¤„ç†ä»»åŠ¡ï¼ˆæŒ‰ä¼˜å…ˆçº§æ’åºï¼‰
            c.execute('''
                SELECT id, job_type, reference_data, priority
                FROM recommendation_jobs
                WHERE job_status = 'pending'
                ORDER BY priority DESC, created_at ASC
                LIMIT 5
            ''')
            
            jobs = c.fetchall()
            
            for job in jobs:
                job_id, job_type, reference_data, priority = job
                
                try:
                    # æ ‡è®°ä»»åŠ¡ä¸ºæ‰§è¡Œä¸­
                    c.execute('''
                        UPDATE recommendation_jobs 
                        SET job_status = 'running', started_at = CURRENT_TIMESTAMP
                        WHERE id = ?
                    ''', (job_id,))
                    conn.commit()
                    
                    print(f"ğŸ”„ å¼€å§‹å¤„ç†æ¨èä»»åŠ¡ {job_id}: {job_type}")
                    
                    # æ ¹æ®ä»»åŠ¡ç±»å‹æ‰§è¡Œç›¸åº”å¤„ç†
                    if job_type == 'full_recompute':
                        self._process_full_recompute(job_id)
                    elif job_type == 'incremental':
                        self._process_incremental_update(job_id, reference_data)
                    elif job_type == 'similar':
                        self._process_similar_recommendations(job_id, reference_data)
                    
                    # æ ‡è®°ä»»åŠ¡å®Œæˆ
                    c.execute('''
                        UPDATE recommendation_jobs 
                        SET job_status = 'completed', completed_at = CURRENT_TIMESTAMP
                        WHERE id = ?
                    ''', (job_id,))
                    conn.commit()
                    
                    print(f"âœ… æ¨èä»»åŠ¡ {job_id} å¤„ç†å®Œæˆ")
                    
                except Exception as e:
                    # æ ‡è®°ä»»åŠ¡å¤±è´¥
                    c.execute('''
                        UPDATE recommendation_jobs 
                        SET job_status = 'failed', 
                            completed_at = CURRENT_TIMESTAMP,
                            error_message = ?
                        WHERE id = ?
                    ''', (str(e), job_id))
                    conn.commit()
                    print(f"âŒ æ¨èä»»åŠ¡ {job_id} å¤„ç†å¤±è´¥: {e}")
                    
        except Exception as e:
            print(f"âŒ å¤„ç†æ¨èä»»åŠ¡å¤±è´¥: {e}")
        finally:
            conn.close()
    
    def _process_full_recompute(self, job_id: int):
        """å¤„ç†å…¨é‡é‡è®¡ç®—ä»»åŠ¡"""
        try:
            # æ¸…é™¤æ—§çš„ä¸ªæ€§åŒ–æ¨èç¼“å­˜
            self._clear_cache('personalized')
            
            # é‡æ–°è®¡ç®—ä¸ªæ€§åŒ–æ¨è
            recommendations = self.ai_recommender.get_personalized_recommendations(limit=50)
            
            if recommendations:
                # ç¼“å­˜æ–°çš„æ¨èç»“æœ
                cache_key = 'personalized_50'
                self._cache_recommendations(cache_key, recommendations, 'personalized')
                
                # åˆ›å»ºä¸åŒlimitçš„ç¼“å­˜ç‰ˆæœ¬
                for limit in [5, 10, 20]:
                    limited_recs = recommendations[:limit]
                    limited_key = f'personalized_{limit}'
                    self._cache_recommendations(limited_key, limited_recs, 'personalized')
                
                print(f"âœ… å…¨é‡é‡è®¡ç®—å®Œæˆï¼Œç¼“å­˜äº† {len(recommendations)} ä¸ªæ¨è")
            else:
                print("âš ï¸ å…¨é‡é‡è®¡ç®—æœªç”Ÿæˆæ¨èç»“æœ")
                
        except Exception as e:
            print(f"âŒ å…¨é‡é‡è®¡ç®—å¤±è´¥: {e}")
            raise
    
    def _process_incremental_update(self, job_id: int, reference_data: str):
        """å¤„ç†å¢é‡æ›´æ–°ä»»åŠ¡"""
        try:
            # è§£æå‚è€ƒæ•°æ®
            data = json.loads(reference_data) if reference_data else {}
            trigger_reason = data.get('trigger_reason', 'unknown')
            
            print(f"ğŸ”„ å¢é‡æ›´æ–°è§¦å‘åŸå› : {trigger_reason}")
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦æ›´æ–°
            if self._should_perform_incremental_update():
                # æ‰§è¡Œå¢é‡æ›´æ–°ï¼ˆå®é™…ä¸Šè¿˜æ˜¯å…¨é‡è®¡ç®—ï¼Œä½†å¯ä»¥ä¼˜åŒ–ï¼‰
                self._process_full_recompute(job_id)
            else:
                print("âš ï¸ è·³è¿‡å¢é‡æ›´æ–°ï¼Œå…´è¶£æ•°æ®æ— æ˜¾è‘—å˜åŒ–")
                
        except Exception as e:
            print(f"âŒ å¢é‡æ›´æ–°å¤±è´¥: {e}")
            raise
    
    def _process_similar_recommendations(self, job_id: int, reference_data: str):
        """å¤„ç†ç›¸ä¼¼è®ºæ–‡æ¨èä»»åŠ¡"""
        try:
            data = json.loads(reference_data) if reference_data else {}
            paper_id = data.get('paper_id')
            limit = data.get('limit', 5)
            
            if not paper_id:
                raise ValueError("ç¼ºå°‘paper_idå‚æ•°")
            
            # è®¡ç®—ç›¸ä¼¼è®ºæ–‡
            similar_papers = self.ai_recommender.find_similar_papers(paper_id, limit=limit)
            
            if similar_papers:
                # ç¼“å­˜ç›¸ä¼¼è®ºæ–‡æ¨è
                cache_key = f'similar_{paper_id}_{limit}'
                self._cache_recommendations(cache_key, similar_papers, 'similar', paper_id)
                
                print(f"âœ… è®ºæ–‡ {paper_id} çš„ç›¸ä¼¼æ¨èè®¡ç®—å®Œæˆï¼Œç¼“å­˜äº† {len(similar_papers)} ä¸ªç»“æœ")
            else:
                print(f"âš ï¸ è®ºæ–‡ {paper_id} æœªæ‰¾åˆ°ç›¸ä¼¼æ¨è")
                
        except Exception as e:
            print(f"âŒ ç›¸ä¼¼æ¨èè®¡ç®—å¤±è´¥: {e}")
            raise
    
    def _cache_recommendations(self, cache_key: str, recommendations: List[Dict], 
                              rec_type: str, reference_paper_id: Optional[int] = None):
        """ç¼“å­˜æ¨èç»“æœ"""
        try:
            conn = self.db.get_connection()
            c = conn.cursor()
            
            # æ¸…é™¤æ—§ç¼“å­˜
            c.execute('DELETE FROM recommendation_cache WHERE cache_key = ?', (cache_key,))
            
            # è®¡ç®—è¿‡æœŸæ—¶é—´
            expires_at = datetime.now() + timedelta(hours=self.CACHE_EXPIRY_HOURS)
            
            # æ’å…¥æ–°ç¼“å­˜
            for i, rec in enumerate(recommendations):
                c.execute('''
                    INSERT INTO recommendation_cache 
                    (cache_key, paper_id, recommendation_type, reference_paper_id,
                     recommendation_score, ai_reason, rank_position, expires_at)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?)
                ''', (
                    cache_key,
                    rec['id'],
                    rec_type,
                    reference_paper_id,
                    rec.get('recommendation_score', 0.0),
                    rec.get('ai_reason', ''),
                    i + 1,
                    expires_at
                ))
            
            conn.commit()
            print(f"ğŸ“¦ ç¼“å­˜ {cache_key}: {len(recommendations)} æ¡æ¨è")
            
        except Exception as e:
            print(f"âŒ ç¼“å­˜æ¨èç»“æœå¤±è´¥: {e}")
            raise
        finally:
            conn.close()
    
    def _clear_cache(self, recommendation_type: str):
        """æ¸…é™¤æŒ‡å®šç±»å‹çš„ç¼“å­˜"""
        try:
            conn = self.db.get_connection()
            c = conn.cursor()
            
            c.execute('''
                DELETE FROM recommendation_cache 
                WHERE recommendation_type = ?
            ''', (recommendation_type,))
            
            conn.commit()
            
        except Exception as e:
            print(f"âŒ æ¸…é™¤ç¼“å­˜å¤±è´¥: {e}")
        finally:
            conn.close()
    
    def _cleanup_expired_cache(self):
        """æ¸…ç†è¿‡æœŸç¼“å­˜"""
        try:
            conn = self.db.get_connection()
            c = conn.cursor()
            
            c.execute('''
                DELETE FROM recommendation_cache 
                WHERE expires_at < CURRENT_TIMESTAMP
            ''')
            
            deleted_count = c.rowcount
            conn.commit()
            
            if deleted_count > 0:
                print(f"ğŸ§¹ æ¸…ç†äº† {deleted_count} æ¡è¿‡æœŸç¼“å­˜")
                
        except Exception as e:
            print(f"âŒ æ¸…ç†è¿‡æœŸç¼“å­˜å¤±è´¥: {e}")
        finally:
            conn.close()
    
    def _check_interest_changes(self):
        """æ£€æŸ¥ç”¨æˆ·å…´è¶£å˜åŒ–ï¼Œè§¦å‘å¢é‡æ›´æ–°"""
        try:
            # è·å–å½“å‰ç”¨æˆ·å…´è¶£æ•°æ®
            current_interests = self._get_current_interests_hash()
            
            # è·å–æœ€è¿‘çš„å¿«ç…§
            conn = self.db.get_connection()
            c = conn.cursor()
            
            c.execute('''
                SELECT snapshot_hash, liked_papers_count
                FROM user_interest_snapshots
                WHERE is_current = TRUE
                ORDER BY created_at DESC
                LIMIT 1
            ''')
            
            last_snapshot = c.fetchone()
            
            # å¦‚æœå…´è¶£å‘ç”Ÿå˜åŒ–ï¼Œè§¦å‘å¢é‡æ›´æ–°
            if not last_snapshot or last_snapshot['snapshot_hash'] != current_interests['hash']:
                
                # æ›´æ–°å¿«ç…§
                self._update_interest_snapshot(current_interests)
                
                # å¦‚æœæœ‰æ˜¾è‘—å˜åŒ–ï¼Œåˆ›å»ºå¢é‡æ›´æ–°ä»»åŠ¡
                if self._is_significant_change(last_snapshot, current_interests):
                    self._create_incremental_job('interest_change')
                    print("ğŸ”„ æ£€æµ‹åˆ°ç”¨æˆ·å…´è¶£æ˜¾è‘—å˜åŒ–ï¼Œè§¦å‘å¢é‡æ›´æ–°")
                    
        except Exception as e:
            print(f"âŒ æ£€æŸ¥å…´è¶£å˜åŒ–å¤±è´¥: {e}")
        finally:
            conn.close()
    
    def _get_current_interests_hash(self) -> Dict:
        """è·å–å½“å‰ç”¨æˆ·å…´è¶£çš„å“ˆå¸Œå€¼"""
        try:
            liked_papers = self.ai_recommender._get_user_liked_papers()
            
            # åˆ›å»ºå…´è¶£æ•°æ®å¿«ç…§
            interests_data = {
                'liked_papers_count': len(liked_papers),
                'liked_papers_ids': sorted([p['id'] for p in liked_papers]),
                'timestamp': datetime.now().isoformat()
            }
            
            # è®¡ç®—å“ˆå¸Œå€¼
            data_str = json.dumps(interests_data, sort_keys=True)
            hash_value = hashlib.md5(data_str.encode()).hexdigest()
            
            return {
                'hash': hash_value,
                'data': interests_data,
                'liked_papers_count': len(liked_papers)
            }
            
        except Exception as e:
            print(f"âŒ è·å–å…´è¶£å“ˆå¸Œå¤±è´¥: {e}")
            return {'hash': '', 'data': {}, 'liked_papers_count': 0}
    
    def _update_interest_snapshot(self, current_interests: Dict):
        """æ›´æ–°ç”¨æˆ·å…´è¶£å¿«ç…§"""
        try:
            conn = self.db.get_connection()
            c = conn.cursor()
            
            # å°†æ—§å¿«ç…§æ ‡è®°ä¸ºéå½“å‰
            c.execute('UPDATE user_interest_snapshots SET is_current = FALSE')
            
            # æ’å…¥æ–°å¿«ç…§
            c.execute('''
                INSERT INTO user_interest_snapshots
                (snapshot_hash, liked_papers_count, snapshot_data)
                VALUES (?, ?, ?)
            ''', (
                current_interests['hash'], 
                current_interests['liked_papers_count'],
                json.dumps(current_interests['data'])
            ))
            
            conn.commit()
            
        except Exception as e:
            print(f"âŒ æ›´æ–°å…´è¶£å¿«ç…§å¤±è´¥: {e}")
        finally:
            conn.close()
    
    def _is_significant_change(self, last_snapshot: Optional[Dict], current_interests: Dict) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºæ˜¾è‘—çš„å…´è¶£å˜åŒ–"""
        if not last_snapshot:
            return True  # é¦–æ¬¡è¿è¡Œ
        
        last_count =last_snapshot.get('liked_papers_count', 0)
        current_count = current_interests['liked_papers_count']
        
        # å¦‚æœå–œçˆ±è®ºæ–‡æ•°é‡å˜åŒ–è¶…è¿‡20%æˆ–å¢åŠ äº†3ç¯‡ä»¥ä¸Šï¼Œè®¤ä¸ºæ˜¯æ˜¾è‘—å˜åŒ–
        if current_count == 0:
            return False
        
        change_ratio = abs(current_count - last_count) / max(current_count, 1)
        change_absolute = abs(current_count - last_count)
        
        return change_ratio > 0.2 or change_absolute >= 3
    
    def _should_perform_incremental_update(self) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥æ‰§è¡Œå¢é‡æ›´æ–°"""
        # è¿™é‡Œå¯ä»¥æ·»åŠ æ›´å¤æ‚çš„é€»è¾‘ï¼Œæ¯”å¦‚æ£€æŸ¥ç¼“å­˜æ˜¯å¦è¿‡æ—§ç­‰
        return True
    
    def _create_incremental_job(self, trigger_reason: str):
        """åˆ›å»ºå¢é‡æ›´æ–°ä»»åŠ¡"""
        try:
            conn = self.db.get_connection()
            c = conn.cursor()
            
            reference_data = json.dumps({
                'trigger_reason': trigger_reason,
                'created_by': 'interest_monitor'
            })
            
            c.execute('''
                INSERT INTO recommendation_jobs
                (job_type, priority, reference_data)
                VALUES ('incremental', 8, ?)
            ''', (reference_data,))
            
            conn.commit()
            
        except Exception as e:
            print(f"âŒ åˆ›å»ºå¢é‡æ›´æ–°ä»»åŠ¡å¤±è´¥: {e}")
        finally:
            conn.close()
    
    # å…¬å¼€æ–¹æ³•ï¼šæ‰‹åŠ¨è§¦å‘ä»»åŠ¡
    
    def create_full_recompute_job(self, priority: int = 7):
        """æ‰‹åŠ¨åˆ›å»ºå…¨é‡é‡è®¡ç®—ä»»åŠ¡"""
        try:
            conn = self.db.get_connection()
            c = conn.cursor()
            
            c.execute('''
                INSERT INTO recommendation_jobs
                (job_type, priority)
                VALUES ('full_recompute', ?)
            ''', (priority,))
            
            conn.commit()
            print("âœ… å…¨é‡é‡è®¡ç®—ä»»åŠ¡å·²åˆ›å»º")
            
        except Exception as e:
            print(f"âŒ åˆ›å»ºå…¨é‡é‡è®¡ç®—ä»»åŠ¡å¤±è´¥: {e}")
        finally:
            conn.close()
    
    def create_similar_job(self, paper_id: int, limit: int = 5, priority: int = 5):
        """æ‰‹åŠ¨åˆ›å»ºç›¸ä¼¼æ¨èä»»åŠ¡"""
        try:
            conn = self.db.get_connection()
            c = conn.cursor()
            
            reference_data = json.dumps({
                'paper_id': paper_id,
                'limit': limit
            })
            
            c.execute('''
                INSERT INTO recommendation_jobs
                (job_type, priority, reference_data)
                VALUES ('similar', ?, ?)
            ''', (priority, reference_data))
            
            conn.commit()
            print(f"âœ… è®ºæ–‡ {paper_id} çš„ç›¸ä¼¼æ¨èä»»åŠ¡å·²åˆ›å»º")
            
        except Exception as e:
            print(f"âŒ åˆ›å»ºç›¸ä¼¼æ¨èä»»åŠ¡å¤±è´¥: {e}")
        finally:
            conn.close()
    
    def get_job_status(self) -> Dict:
        """è·å–ä»»åŠ¡å¤„ç†çŠ¶æ€"""
        try:
            conn = self.db.get_connection()
            c = conn.cursor()
            
            c.execute('''
                SELECT job_status, COUNT(*) as count
                FROM recommendation_jobs
                GROUP BY job_status
            ''')
            
            status_counts = dict(c.fetchall())
            
            c.execute('''
                SELECT COUNT(*) as cache_count,
                       MIN(expires_at) as earliest_expiry
                FROM recommendation_cache
                WHERE expires_at > CURRENT_TIMESTAMP
            ''')
            
            cache_info = c.fetchone()
            
            return {
                'processor_running': self.is_running,
                'job_counts': status_counts,
                'cache_count': cache_info['cache_count'] if cache_info else 0,
                'earliest_cache_expiry': cache_info['earliest_expiry'] if cache_info else None
            }
            
        except Exception as e:
            print(f"âŒ è·å–ä»»åŠ¡çŠ¶æ€å¤±è´¥: {e}")
            return {}
        finally:
            conn.close()


# å…¨å±€å¤„ç†å™¨å®ä¾‹
recommendation_processor = AsyncRecommendationProcessor()