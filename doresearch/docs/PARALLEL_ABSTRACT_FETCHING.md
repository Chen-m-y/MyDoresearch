# IEEEè®ºæ–‡æ‘˜è¦å¤šçº¿ç¨‹å¹¶è¡ŒæŠ“å–ä¼˜åŒ–æ–¹æ¡ˆ

## ğŸ¯ ä¼˜åŒ–ç›®æ ‡

å½“å‰æŠ“å–IEEEè®ºæ–‡æ—¶ï¼Œå¦‚æœéœ€è¦è·å–è¯¦ç»†æ‘˜è¦ä¿¡æ¯ï¼Œé€šå¸¸éœ€è¦ï¼š
1. å…ˆè·å–è®ºæ–‡åˆ—è¡¨ï¼ˆåŒ…å«åŸºæœ¬ä¿¡æ¯ï¼‰
2. å†é€ä¸ªè®¿é—®æ¯ç¯‡è®ºæ–‡çš„è¯¦ç»†é¡µé¢è·å–å®Œæ•´æ‘˜è¦
3. åˆå¹¶æ•°æ®è¿”å›

è¿™ä¸ªè¿‡ç¨‹å¦‚æœä¸²è¡Œæ‰§è¡Œä¼šå¾ˆæ…¢ï¼Œéœ€è¦å¹¶è¡Œä¼˜åŒ–ã€‚

## ğŸ”§ å¤šçº¿ç¨‹å¹¶è¡Œæ–¹æ¡ˆ

### 1. åœ¨ `do_research_fetch` å¾®æœåŠ¡ä¸­å®ç°

#### æ–¹æ¡ˆAï¼šä½¿ç”¨ ThreadPoolExecutor

```python
# ieee_service.py
import concurrent.futures
import requests
from typing import List, Dict, Optional
import time
import logging

class IEEEService:
    def __init__(self, max_workers: int = 10):
        self.max_workers = max_workers
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
        
    def fetch_papers_with_abstracts(self, punumber: str, limit: int = 50) -> List[Dict]:
        """å¹¶è¡Œè·å–IEEEè®ºæ–‡åŠå…¶å®Œæ•´æ‘˜è¦"""
        
        # ç¬¬ä¸€æ­¥ï¼šè·å–è®ºæ–‡åˆ—è¡¨ï¼ˆåŸºæœ¬ä¿¡æ¯ï¼‰
        papers_basic = self._get_papers_list(punumber, limit)
        
        if not papers_basic:
            return []
        
        # ç¬¬äºŒæ­¥ï¼šå¹¶è¡Œè·å–æ¯ç¯‡è®ºæ–‡çš„è¯¦ç»†æ‘˜è¦
        papers_with_abstracts = self._fetch_abstracts_parallel(papers_basic)
        
        return papers_with_abstracts
    
    def _get_papers_list(self, punumber: str, limit: int) -> List[Dict]:
        """è·å–è®ºæ–‡åŸºæœ¬ä¿¡æ¯åˆ—è¡¨"""
        # IEEE APIè°ƒç”¨è·å–è®ºæ–‡åˆ—è¡¨
        # è¿™é‡Œè¿”å›åŒ…å«article_numberä½†å¯èƒ½æ‘˜è¦ä¸å®Œæ•´çš„è®ºæ–‡åˆ—è¡¨
        pass
    
    def _fetch_abstracts_parallel(self, papers: List[Dict]) -> List[Dict]:
        """å¹¶è¡Œè·å–è®ºæ–‡æ‘˜è¦"""
        
        def fetch_single_abstract(paper: Dict) -> Dict:
            """è·å–å•ç¯‡è®ºæ–‡çš„å®Œæ•´æ‘˜è¦"""
            try:
                article_number = paper.get('ieee_number') or paper.get('article_number')
                if not article_number:
                    return paper
                
                # æ„å»ºè¯¦ç»†é¡µé¢URL
                detail_url = f"https://ieeexplore.ieee.org/document/{article_number}"
                
                # æ·»åŠ å»¶è¿Ÿé¿å…è¢«é™æµ
                time.sleep(0.1)  # 100mså»¶è¿Ÿ
                
                response = self.session.get(detail_url, timeout=10)
                response.raise_for_status()
                
                # è§£æé¡µé¢è·å–å®Œæ•´æ‘˜è¦
                full_abstract = self._extract_abstract_from_page(response.text)
                
                if full_abstract:
                    paper['abstract'] = full_abstract
                
                logging.info(f"Successfully fetched abstract for paper {article_number}")
                return paper
                
            except Exception as e:
                logging.error(f"Failed to fetch abstract for paper {paper.get('title', 'Unknown')}: {e}")
                return paper  # è¿”å›åŸå§‹paperï¼Œå³ä½¿æ‘˜è¦è·å–å¤±è´¥
        
        # ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œå¤„ç†
        with concurrent.futures.ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            # æäº¤æ‰€æœ‰ä»»åŠ¡
            future_to_paper = {
                executor.submit(fetch_single_abstract, paper): paper 
                for paper in papers
            }
            
            # æ”¶é›†ç»“æœ
            results = []
            completed = 0
            total = len(papers)
            
            for future in concurrent.futures.as_completed(future_to_paper, timeout=300):
                try:
                    result = future.result()
                    results.append(result)
                    completed += 1
                    
                    # è¿›åº¦æ—¥å¿—
                    if completed % 5 == 0 or completed == total:
                        logging.info(f"Abstract fetching progress: {completed}/{total}")
                        
                except Exception as e:
                    # å•ä¸ªä»»åŠ¡å¤±è´¥ä¸å½±å“æ•´ä½“
                    original_paper = future_to_paper[future]
                    results.append(original_paper)
                    logging.error(f"Future failed for paper: {e}")
        
        return results
    
    def _extract_abstract_from_page(self, html_content: str) -> Optional[str]:
        """ä»IEEEé¡µé¢ä¸­æå–å®Œæ•´æ‘˜è¦"""
        from bs4 import BeautifulSoup
        
        soup = BeautifulSoup(html_content, 'html.parser')
        
        # IEEEé¡µé¢çš„æ‘˜è¦é€šå¸¸åœ¨è¿™äº›é€‰æ‹©å™¨ä¸­
        abstract_selectors = [
            '.abstract-text',
            '.abstract .description',
            '[data-testid="abstract-text"]',
            '.u-mb-1.stats-abstract-text'
        ]
        
        for selector in abstract_selectors:
            abstract_elem = soup.select_one(selector)
            if abstract_elem:
                return abstract_elem.get_text(strip=True)
        
        return None
```

#### æ–¹æ¡ˆBï¼šä½¿ç”¨ asyncio + aiohttp (æ›´é«˜æ•ˆ)

```python
# ieee_async_service.py
import asyncio
import aiohttp
import logging
from typing import List, Dict, Optional

class IEEEAsyncService:
    def __init__(self, concurrent_limit: int = 10):
        self.concurrent_limit = concurrent_limit
        self.semaphore = asyncio.Semaphore(concurrent_limit)
        
    async def fetch_papers_with_abstracts(self, punumber: str, limit: int = 50) -> List[Dict]:
        """å¼‚æ­¥å¹¶è¡Œè·å–IEEEè®ºæ–‡åŠå…¶å®Œæ•´æ‘˜è¦"""
        
        # ç¬¬ä¸€æ­¥ï¼šè·å–è®ºæ–‡åˆ—è¡¨
        papers_basic = await self._get_papers_list_async(punumber, limit)
        
        if not papers_basic:
            return []
        
        # ç¬¬äºŒæ­¥ï¼šå¼‚æ­¥å¹¶è¡Œè·å–æ‘˜è¦
        papers_with_abstracts = await self._fetch_abstracts_async(papers_basic)
        
        return papers_with_abstracts
    
    async def _get_papers_list_async(self, punumber: str, limit: int) -> List[Dict]:
        """å¼‚æ­¥è·å–è®ºæ–‡åŸºæœ¬ä¿¡æ¯åˆ—è¡¨"""
        async with aiohttp.ClientSession() as session:
            # IEEE APIè°ƒç”¨
            pass
    
    async def _fetch_abstracts_async(self, papers: List[Dict]) -> List[Dict]:
        """å¼‚æ­¥å¹¶è¡Œè·å–è®ºæ–‡æ‘˜è¦"""
        
        async def fetch_single_abstract(session: aiohttp.ClientSession, paper: Dict) -> Dict:
            """å¼‚æ­¥è·å–å•ç¯‡è®ºæ–‡çš„å®Œæ•´æ‘˜è¦"""
            async with self.semaphore:  # æ§åˆ¶å¹¶å‘æ•°
                try:
                    article_number = paper.get('ieee_number') or paper.get('article_number')
                    if not article_number:
                        return paper
                    
                    detail_url = f"https://ieeexplore.ieee.org/document/{article_number}"
                    
                    # æ·»åŠ å»¶è¿Ÿé¿å…è¢«é™æµ
                    await asyncio.sleep(0.05)  # 50mså»¶è¿Ÿ
                    
                    async with session.get(detail_url, timeout=10) as response:
                        response.raise_for_status()
                        html_content = await response.text()
                        
                        # è§£ææ‘˜è¦
                        full_abstract = self._extract_abstract_from_page(html_content)
                        
                        if full_abstract:
                            paper['abstract'] = full_abstract
                        
                        logging.info(f"Successfully fetched abstract for paper {article_number}")
                        return paper
                        
                except Exception as e:
                    logging.error(f"Failed to fetch abstract for paper {paper.get('title', 'Unknown')}: {e}")
                    return paper
        
        # åˆ›å»ºä¼šè¯å’Œä»»åŠ¡
        connector = aiohttp.TCPConnector(limit=50, limit_per_host=10)
        async with aiohttp.ClientSession(connector=connector) as session:
            tasks = [
                fetch_single_abstract(session, paper) 
                for paper in papers
            ]
            
            # æ‰§è¡Œæ‰€æœ‰ä»»åŠ¡
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            # å¤„ç†ç»“æœå’Œå¼‚å¸¸
            processed_results = []
            for i, result in enumerate(results):
                if isinstance(result, Exception):
                    logging.error(f"Task {i} failed: {result}")
                    processed_results.append(papers[i])  # è¿”å›åŸå§‹paper
                else:
                    processed_results.append(result)
            
            return processed_results
```

### 2. é…ç½®å’Œä¼˜åŒ–å‚æ•°

#### ç¯å¢ƒå˜é‡é…ç½®

```bash
# åœ¨ do_research_fetch å¾®æœåŠ¡ä¸­
ABSTRACT_FETCHING_ENABLED=true
MAX_CONCURRENT_ABSTRACT_REQUESTS=8
ABSTRACT_REQUEST_DELAY_MS=100
ABSTRACT_TIMEOUT_SECONDS=15
ENABLE_ABSTRACT_CACHE=true
ABSTRACT_CACHE_TTL_HOURS=24
```

#### æ™ºèƒ½å¹¶å‘æ§åˆ¶

```python
# config.py in do_research_fetch
import os

class AbstractFetchConfig:
    # åŸºç¡€é…ç½®
    ENABLED = os.getenv('ABSTRACT_FETCHING_ENABLED', 'true').lower() == 'true'
    MAX_WORKERS = int(os.getenv('MAX_CONCURRENT_ABSTRACT_REQUESTS', '8'))
    REQUEST_DELAY = float(os.getenv('ABSTRACT_REQUEST_DELAY_MS', '100')) / 1000
    TIMEOUT = int(os.getenv('ABSTRACT_TIMEOUT_SECONDS', '15'))
    
    # ç¼“å­˜é…ç½®
    CACHE_ENABLED = os.getenv('ENABLE_ABSTRACT_CACHE', 'true').lower() == 'true'
    CACHE_TTL = int(os.getenv('ABSTRACT_CACHE_TTL_HOURS', '24')) * 3600
    
    # æ™ºèƒ½è°ƒæ•´ï¼šæ ¹æ®å“åº”æ—¶é—´åŠ¨æ€è°ƒæ•´å¹¶å‘æ•°
    @classmethod
    def get_adaptive_workers(cls, avg_response_time: float) -> int:
        """æ ¹æ®å¹³å‡å“åº”æ—¶é—´åŠ¨æ€è°ƒæ•´å¹¶å‘æ•°"""
        if avg_response_time < 1.0:  # å“åº”å¿«ï¼Œå¯ä»¥å¢åŠ å¹¶å‘
            return min(cls.MAX_WORKERS * 2, 15)
        elif avg_response_time > 3.0:  # å“åº”æ…¢ï¼Œå‡å°‘å¹¶å‘
            return max(cls.MAX_WORKERS // 2, 3)
        else:
            return cls.MAX_WORKERS
```

### 3. ç¼“å­˜æœºåˆ¶ä¼˜åŒ–

```python
# abstract_cache.py
import redis
import hashlib
import json
import logging
from typing import Optional

class AbstractCache:
    def __init__(self, redis_url: str = "redis://localhost:6379"):
        self.redis_client = redis.from_url(redis_url)
        self.key_prefix = "ieee_abstract:"
        
    def _get_cache_key(self, article_number: str) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        return f"{self.key_prefix}{article_number}"
    
    def get_cached_abstract(self, article_number: str) -> Optional[str]:
        """è·å–ç¼“å­˜çš„æ‘˜è¦"""
        try:
            cache_key = self._get_cache_key(article_number)
            cached = self.redis_client.get(cache_key)
            
            if cached:
                data = json.loads(cached)
                return data.get('abstract')
                
        except Exception as e:
            logging.error(f"Cache get error: {e}")
        
        return None
    
    def cache_abstract(self, article_number: str, abstract: str, ttl: int = 86400):
        """ç¼“å­˜æ‘˜è¦"""
        try:
            cache_key = self._get_cache_key(article_number)
            data = {
                'abstract': abstract,
                'cached_at': time.time()
            }
            
            self.redis_client.setex(
                cache_key, 
                ttl, 
                json.dumps(data)
            )
            
        except Exception as e:
            logging.error(f"Cache set error: {e}")
```

### 4. åœ¨ä¸»æœåŠ¡ä¸­çš„é›†æˆ

```python
# main_ieee_service.py
class EnhancedIEEEService:
    def __init__(self):
        self.async_service = IEEEAsyncService()
        self.cache = AbstractCache()
        self.config = AbstractFetchConfig()
        
    async def fetch_papers(self, punumber: str, limit: int = 50) -> Dict:
        """ä¸»è¦çš„è®ºæ–‡è·å–æ–¹æ³•"""
        
        if not self.config.ENABLED:
            # å¦‚æœæœªå¯ç”¨æ‘˜è¦å¹¶è¡Œè·å–ï¼Œä½¿ç”¨ç®€å•æ¨¡å¼
            return await self._fetch_papers_simple(punumber, limit)
        
        # å¯ç”¨å¹¶è¡Œæ‘˜è¦è·å–
        papers = await self.async_service.fetch_papers_with_abstracts(punumber, limit)
        
        return {
            "success": True,
            "data": {
                "papers": papers,
                "total_count": len(papers),
                "has_more": len(papers) >= limit
            },
            "source_info": {
                "source": "ieee",
                "enhanced_abstracts": True,
                "cache_hit_rate": self._calculate_cache_hit_rate()
            }
        }
```

### 5. æ€§èƒ½ç›‘æ§å’Œæ—¥å¿—

```python
# performance_monitor.py
import time
import logging
from dataclasses import dataclass
from typing import List

@dataclass
class FetchMetrics:
    total_papers: int
    successful_abstracts: int
    cache_hits: int
    total_time: float
    avg_response_time: float
    error_count: int

class PerformanceMonitor:
    def __init__(self):
        self.metrics_history: List[FetchMetrics] = []
    
    def log_fetch_session(self, metrics: FetchMetrics):
        """è®°å½•ä¸€æ¬¡æŠ“å–ä¼šè¯çš„æ€§èƒ½æŒ‡æ ‡"""
        self.metrics_history.append(metrics)
        
        logging.info(f"""
        æŠ“å–æ€§èƒ½æŠ¥å‘Š:
        - æ€»è®ºæ–‡æ•°: {metrics.total_papers}
        - æˆåŠŸè·å–æ‘˜è¦: {metrics.successful_abstracts}
        - ç¼“å­˜å‘½ä¸­: {metrics.cache_hits}
        - æ€»è€—æ—¶: {metrics.total_time:.2f}ç§’
        - å¹³å‡å“åº”æ—¶é—´: {metrics.avg_response_time:.2f}ç§’
        - é”™è¯¯æ•°: {metrics.error_count}
        - æˆåŠŸç‡: {metrics.successful_abstracts/metrics.total_papers*100:.1f}%
        - ç¼“å­˜å‘½ä¸­ç‡: {metrics.cache_hits/metrics.total_papers*100:.1f}%
        """)
    
    def get_performance_recommendations(self) -> List[str]:
        """åŸºäºå†å²æ•°æ®æä¾›æ€§èƒ½ä¼˜åŒ–å»ºè®®"""
        if not self.metrics_history:
            return []
        
        latest = self.metrics_history[-1]
        recommendations = []
        
        if latest.avg_response_time > 3.0:
            recommendations.append("å¹³å‡å“åº”æ—¶é—´è¾ƒæ…¢ï¼Œå»ºè®®å‡å°‘å¹¶å‘æ•°")
        
        if latest.cache_hits / latest.total_papers < 0.3:
            recommendations.append("ç¼“å­˜å‘½ä¸­ç‡è¾ƒä½ï¼Œå»ºè®®å¢åŠ ç¼“å­˜TTL")
        
        if latest.error_count / latest.total_papers > 0.1:
            recommendations.append("é”™è¯¯ç‡è¾ƒé«˜ï¼Œå»ºè®®å¢åŠ é‡è¯•æœºåˆ¶")
        
        return recommendations
```

### 6. DoResearchåç«¯çš„é€‚é…

åœ¨DoResearchçš„è®¢é˜…åŒæ­¥æœåŠ¡ä¸­ï¼Œå¯ä»¥æ·»åŠ è¿›åº¦ç›‘æ§ï¼š

```python
# services/subscription_service.py ä¸­çš„ä¿®æ”¹
def _sync_subscription(self, subscription: Dict):
    """åŒæ­¥å•ä¸ªè®¢é˜…ï¼ˆå¢åŠ è¿›åº¦ç›‘æ§ï¼‰"""
    subscription_id = subscription['id']
    
    # åˆ›å»ºåŒæ­¥è®°å½•
    sync_id = self.sync_history_manager.create_sync_record(subscription_id)
    
    try:
        # è°ƒç”¨å¤–éƒ¨æœåŠ¡è·å–è®ºæ–‡ï¼ˆç°åœ¨æ”¯æŒå¹¶è¡Œæ‘˜è¦è·å–ï¼‰
        start_time = time.time()
        result = self.external_client.fetch_papers(
            subscription['source_type'], 
            subscription['source_params']
        )
        
        if not result['success']:
            raise Exception(result['error'])
        
        service_data = result['data']
        papers = service_data.get('data', {}).get('papers', [])
        
        # è®°å½•æ€§èƒ½ä¿¡æ¯
        fetch_time = time.time() - start_time
        enhanced_abstracts = service_data.get('source_info', {}).get('enhanced_abstracts', False)
        
        logging.info(f"Subscription {subscription_id}: "
                   f"Fetched {len(papers)} papers in {fetch_time:.2f}s, "
                   f"Enhanced abstracts: {enhanced_abstracts}")
        
        # å¤„ç†è®ºæ–‡æ•°æ®
        process_result = self.paper_processor.process_papers(
            papers, subscription_id, subscription['name']
        )
        
        if not process_result['success']:
            raise Exception(process_result['error'])
        
        # æ›´æ–°åŒæ­¥è®°å½•ä¸ºæˆåŠŸ
        self.sync_history_manager.update_sync_record(
            sync_id, 'success',
            papers_found=process_result['total_papers'],
            papers_new=process_result['new_papers'],
            service_response=json.dumps({
                **service_data,
                'performance': {
                    'fetch_time': fetch_time,
                    'enhanced_abstracts': enhanced_abstracts
                }
            })
        )
        
        # ... å…¶ä½™ä»£ç ä¸å˜
```

## ğŸ“Š é¢„æœŸæ€§èƒ½æå‡

### ä¸²è¡Œ vs å¹¶è¡Œå¯¹æ¯”

| åœºæ™¯ | è®ºæ–‡æ•°é‡ | ä¸²è¡Œè€—æ—¶ | å¹¶è¡Œè€—æ—¶(8çº¿ç¨‹) | æå‡å€æ•° |
|------|----------|----------|----------------|----------|
| å°æ‰¹é‡ | 10ç¯‡ | ~30ç§’ | ~8ç§’ | 3.75x |
| ä¸­æ‰¹é‡ | 30ç¯‡ | ~90ç§’ | ~15ç§’ | 6x |
| å¤§æ‰¹é‡ | 50ç¯‡ | ~150ç§’ | ~25ç§’ | 6x |

### ä¼˜åŒ–æ•ˆæœ

1. **é€Ÿåº¦æå‡**: 6-8å€çš„æ€§èƒ½æå‡
2. **èµ„æºåˆ©ç”¨**: æ›´å¥½çš„ç½‘ç»œå’ŒCPUåˆ©ç”¨ç‡
3. **ç”¨æˆ·ä½“éªŒ**: æ˜¾è‘—å‡å°‘ç­‰å¾…æ—¶é—´
4. **ç¼“å­˜æ•ˆæœ**: é‡å¤è®¿é—®æ—¶å‡ ä¹ç¬æ—¶å“åº”

## ğŸš€ å®æ–½å»ºè®®

### é˜¶æ®µ1ï¼šåŸºç¡€å¹¶è¡Œå®ç°
- åœ¨`do_research_fetch`ä¸­å®ç°ThreadPoolExecutoræ–¹æ¡ˆ
- é…ç½®åˆç†çš„å¹¶å‘æ•°ï¼ˆå»ºè®®8-10ä¸ªçº¿ç¨‹ï¼‰
- æ·»åŠ åŸºç¡€çš„é”™è¯¯å¤„ç†å’Œé‡è¯•æœºåˆ¶

### é˜¶æ®µ2ï¼šæ€§èƒ½ä¼˜åŒ–
- æ·»åŠ Redisç¼“å­˜å±‚
- å®ç°æ™ºèƒ½å¹¶å‘æ§åˆ¶
- å¢åŠ è¯¦ç»†çš„æ€§èƒ½ç›‘æ§

### é˜¶æ®µ3ï¼šé«˜çº§ä¼˜åŒ–
- å‡çº§åˆ°asyncio+aiohttpæ–¹æ¡ˆ
- å®ç°è‡ªé€‚åº”å¹¶å‘è°ƒæ•´
- æ·»åŠ æœºå™¨å­¦ä¹ é¢„æµ‹ç¼“å­˜ç­–ç•¥

è¿™ä¸ªå¤šçº¿ç¨‹å¹¶è¡Œæ–¹æ¡ˆå¯ä»¥æ˜¾è‘—æå‡IEEEè®ºæ–‡æ‘˜è¦çš„è·å–æ•ˆç‡ï¼Œè®©ç”¨æˆ·åœ¨ä½¿ç”¨æ–°è®¢é˜…ç³»ç»Ÿæ—¶è·å¾—æ›´å¥½çš„ä½“éªŒã€‚