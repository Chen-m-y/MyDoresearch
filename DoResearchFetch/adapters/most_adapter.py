"""
å›½å®¶ç§‘æŠ€ç®¡ç†ä¿¡æ¯ç³»ç»Ÿé€‚é…å™¨
åŸºäºç§‘æŠ€éƒ¨é€šçŸ¥å…¬å‘Šé¡µé¢æŠ“å–é¡¹ç›®ç”³æŠ¥æ–°é—»
"""

import re
from typing import Dict, List, Any, Optional
from datetime import datetime
from bs4 import BeautifulSoup
from .base import BaseNewsAdapter, NewsData
from utils.exceptions import FetchError, ValidationError
from utils.progress_monitor import create_progress_tracker


class MOSTAdapter(BaseNewsAdapter):
    """å›½å®¶ç§‘æŠ€ç®¡ç†ä¿¡æ¯ç³»ç»Ÿï¼ˆç§‘æŠ€éƒ¨ï¼‰æ–°é—»é€‚é…å™¨"""

    @property
    def name(self) -> str:
        return "most"

    @property
    def display_name(self) -> str:
        return "å›½å®¶ç§‘æŠ€ç®¡ç†ä¿¡æ¯ç³»ç»Ÿ"

    @property
    def description(self) -> str:
        return "ç§‘æŠ€éƒ¨é€šçŸ¥å…¬å‘Šï¼ŒåŒ…å«å›½å®¶ç§‘æŠ€é¡¹ç›®ç”³æŠ¥ä¿¡æ¯"

    @property
    def required_params(self) -> List[str]:
        return []  # æ— å¿…éœ€å‚æ•°

    @property
    def optional_params(self) -> List[str]:
        return ["limit", "category_filter", "date_from", "date_to"]

    def _validate_specific_params(self, params: Dict[str, Any]) -> None:
        """ç§‘æŠ€éƒ¨ç‰¹å®šçš„å‚æ•°éªŒè¯"""
        limit = params.get('limit', 20)
        if not isinstance(limit, int) or limit <= 0 or limit > 100:
            raise ValidationError("limitå¿…é¡»æ˜¯1-100ä¹‹é—´çš„æ•´æ•°")

    def fetch_news(self, params: Dict[str, Any]) -> Dict[str, Any]:
        """
        æŠ“å–ç§‘æŠ€éƒ¨æ–°é—»æ•°æ®

        Args:
            params: åŒ…å«å¯é€‰å‚æ•°çš„å­—å…¸
                - limit: è¿”å›ç»“æœæ•°é‡é™åˆ¶ (é»˜è®¤20, æœ€å¤§100)
                - category_filter: åˆ†ç±»è¿‡æ»¤ (funding_announcement, policy_updateç­‰)
                - date_from: å¼€å§‹æ—¥æœŸ (YYYY-MM-DD)
                - date_to: ç»“æŸæ—¥æœŸ (YYYY-MM-DD)
        """
        limit = params.get('limit', 20)
        category_filter = params.get('category_filter')
        date_from = params.get('date_from')
        date_to = params.get('date_to')

        # å®Œå…¨ç§»é™¤è¿›åº¦ç›‘æ§ï¼Œç›´æ¥æ‰§è¡Œ
        try:
            import time
            total_start = time.time()

            print(f"ğŸ” å¼€å§‹æŠ“å–ç§‘æŠ€éƒ¨æ–°é—»...")

            # 1. è·å–ä¸»é¡µæ•°æ®
            fetch_start = time.time()
            news_list = self._fetch_news_list()
            fetch_time = time.time() - fetch_start
            print(f"â±ï¸  æŠ“å–æ–°é—»åˆ—è¡¨è€—æ—¶: {fetch_time:.2f}ç§’, è·å–åˆ° {len(news_list)} æ¡")

            # 2. è½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼
            news_items = []
            for i, raw_news in enumerate(news_list):
                try:
                    news_data = self._map_to_news_data(raw_news)

                    # åº”ç”¨è¿‡æ»¤æ¡ä»¶
                    if self._should_include_news(news_data, category_filter, date_from, date_to):
                        news_items.append(news_data)

                    # tracker.increment(success=True, operation=f"å¤„ç†æ–°é—» {i+1}/{len(news_list)}")
                    pass
                except Exception as e:
                    print(f"Warning: Failed to process news item: {e}")
                    # tracker.increment(success=False, operation=f"å¤„ç†æ–°é—»å¤±è´¥ {i+1}/{len(news_list)}")
                    continue

            # tracker.update(operation="æ’åºå’Œåº”ç”¨é™åˆ¶...")

            # 3. æŒ‰å‘å¸ƒæ—¥æœŸå€’åºæ’åº
            news_items.sort(key=lambda x: x.published_date, reverse=True)

            # 4. åº”ç”¨limité™åˆ¶
            news_items = news_items[:limit]

            # è½¬æ¢ä¸ºå­—å…¸æ ¼å¼
            news_dicts = [news.to_dict() for news in news_items]

            # 5. æ„å»ºå“åº”
            total_count = len(news_dicts)
            has_more = len(news_list) > limit

            result = {
                'news': news_dicts,
                'total_count': total_count,
                'has_more': has_more,
                'next_cursor': None,  # ç§‘æŠ€éƒ¨ç½‘ç«™é€šå¸¸ä¸æ”¯æŒåˆ†é¡µæ¸¸æ ‡
                'rate_limit_remaining': None,
                'cache_hit': False
            }

            # è®¾ç½®ç»“æœåˆ°è¿›åº¦è¿½è¸ªå™¨
            # tracker.set_results({
            #     'news_count': len(news_dicts),
            #     'total_available': len(news_list),
            #     'filtered': bool(category_filter or date_from or date_to)
            # })

            total_time = time.time() - total_start
            print(f"ğŸ‰ ç§‘æŠ€éƒ¨æ–°é—»æŠ“å–å®Œæˆï¼Œæ€»è€—æ—¶: {total_time:.2f}ç§’")

            # ä¸´æ—¶æ·»åŠ è°ƒè¯•ä¿¡æ¯åˆ°è¿”å›ç»“æœä¸­
            result['debug_info'] = {
                'fetch_time': f"{fetch_time:.2f}s",
                'total_time': f"{total_time:.2f}s",
                'news_count': len(news_list)
            }

            return result

        except FetchError:
            raise
        except Exception as e:
            raise FetchError(
                f"ç§‘æŠ€éƒ¨æ–°é—»æŠ“å–å¤±è´¥: {str(e)}",
                error_code='MOST_FETCH_ERROR'
            )

    def _fetch_news_list(self) -> List[Dict[str, Any]]:
        """è·å–æ–°é—»åˆ—è¡¨"""
        url = "https://service.most.gov.cn/kjjh_tztg/"

        try:
            # ä½¿ç”¨çˆ¶ç±»çš„è¯·æ±‚æ–¹æ³•
            response = self._make_request_html(url)
            soup = BeautifulSoup(response, 'html.parser')

            items = []

            # æŸ¥æ‰¾æ‰€æœ‰è¡¨æ ¼è¡Œ
            rows = soup.find_all('tr')

            for row in rows:
                try:
                    # æå–æ ‡é¢˜
                    title_cell = row.find('td', class_='table_gkgs_title')
                    if not title_cell:
                        continue

                    title = title_cell.get_text(strip=True)
                    if not title:
                        continue

                    # æå–é“¾æ¥
                    title_div = title_cell.find('div')
                    if not title_div or not title_div.get('onclick'):
                        continue

                    onclick = title_div.get('onclick')
                    link_match = re.search(r"['\"](.*?)['\"]", onclick)
                    if not link_match:
                        continue

                    link = link_match.group(1)
                    if not link.startswith('http'):
                        link = f"https://service.most.gov.cn{link}" if link.startswith('/') else f"https://service.most.gov.cn/kjjh_tztg/{link}"

                    # æå–å‘å¸ƒå•ä½
                    unit_cell = row.find('td', class_='table_gkgs_unit')
                    unit = unit_cell.get_text(strip=True) if unit_cell else "ç§‘æŠ€éƒ¨"

                    # æå–æ—¥æœŸ
                    date_cell = row.find('td', class_='table_gkgs_date')
                    date_str = date_cell.get_text(strip=True) if date_cell else ""

                    items.append({
                        'title': title,
                        'link': link,
                        'unit': unit,
                        'date': date_str,
                        'raw_html': str(row)
                    })

                except Exception as e:
                    print(f"Warning: Failed to parse row: {e}")
                    continue

            return items

        except Exception as e:
            raise FetchError(
                f"è·å–ç§‘æŠ€éƒ¨æ–°é—»åˆ—è¡¨å¤±è´¥: {str(e)}",
                error_code='NEWS_LIST_FETCH_ERROR'
            )

    def _map_to_news_data(self, raw_data: Dict[str, Any]) -> NewsData:
        """å°†ç§‘æŠ€éƒ¨åŸå§‹æ•°æ®æ˜ å°„ä¸ºæ ‡å‡†æ ¼å¼"""

        title = raw_data.get('title', '').strip()
        link = raw_data.get('link', '')
        unit = raw_data.get('unit', 'ç§‘æŠ€éƒ¨')
        date_str = raw_data.get('date', '')

        # è§£æå‘å¸ƒæ—¥æœŸ
        published_date = self._parse_date(date_str)

        # ç”Ÿæˆæ‘˜è¦ï¼ˆä»æ ‡é¢˜æå–å…³é”®ä¿¡æ¯ï¼‰
        summary = self._generate_summary(title)

        # ç¡®å®šæ–°é—»åˆ†ç±»
        category = self._categorize_news(title)

        # æå–å…³é”®è¯
        keywords = self._extract_keywords(title)

        # ç¡®å®šä¼˜å…ˆçº§
        priority = self._determine_priority(title, category)

        # åˆ¤æ–­çŠ¶æ€ï¼ˆåŸºäºæ—¥æœŸå’Œå†…å®¹ï¼‰
        status = self._determine_status(title, published_date)

        # ç§‘æŠ€éƒ¨ç‰¹å®šå­—æ®µ
        source_specific = {
            'source_unit': unit,
            'original_link': link,
            'news_source': 'å›½å®¶ç§‘æŠ€ç®¡ç†ä¿¡æ¯ç³»ç»Ÿ'
        }

        # å…ƒæ•°æ®
        metadata = {
            'publisher': 'ä¸­åäººæ°‘å…±å’Œå›½ç§‘å­¦æŠ€æœ¯éƒ¨',
            'website': 'https://service.most.gov.cn/',
            'crawl_time': datetime.now().isoformat(),
            'data_source': 'most_kjjh_tztg'
        }

        return NewsData(
            title=title,
            content=title,  # åˆå§‹å†…å®¹ä¸ºæ ‡é¢˜ï¼Œå¯åç»­å¢å¼º
            summary=summary,
            source='most',
            published_date=published_date,
            url=link,
            category=category,
            organization=unit,
            keywords=keywords,
            priority=priority,
            status=status,
            source_specific=source_specific,
            metadata=metadata
        )

    def _parse_date(self, date_str: str) -> str:
        """è§£ææ—¥æœŸå­—ç¬¦ä¸²"""
        if not date_str:
            return datetime.now().strftime('%Y-%m-%d %H:%M:%S')

        # å¸¸è§çš„æ—¥æœŸæ ¼å¼
        date_patterns = [
            r'(\d{4})-(\d{2})-(\d{2})',  # YYYY-MM-DD
            r'(\d{4})/(\d{2})/(\d{2})',  # YYYY/MM/DD
            r'(\d{4})\.(\d{2})\.(\d{2})',  # YYYY.MM.DD
            r'(\d{4})å¹´(\d{1,2})æœˆ(\d{1,2})æ—¥',  # YYYYå¹´MMæœˆDDæ—¥
        ]

        for pattern in date_patterns:
            match = re.search(pattern, date_str)
            if match:
                groups = match.groups()
                year, month, day = groups[0], groups[1].zfill(2), groups[2].zfill(2)
                try:
                    # éªŒè¯æ—¥æœŸæœ‰æ•ˆæ€§
                    datetime(int(year), int(month), int(day))
                    return f"{year}-{month}-{day} 00:00:00"
                except ValueError:
                    continue

        # å¦‚æœæ— æ³•è§£æï¼Œè¿”å›å½“å‰æ—¥æœŸ
        return datetime.now().strftime('%Y-%m-%d %H:%M:%S')

    def _generate_summary(self, title: str) -> str:
        """ä»æ ‡é¢˜ç”Ÿæˆæ‘˜è¦"""
        # ç®€å•çš„æ‘˜è¦ç”Ÿæˆé€»è¾‘
        if len(title) <= 50:
            return title

        # æˆªå–å‰50ä¸ªå­—ç¬¦å¹¶æ·»åŠ çœç•¥å·
        return title[:47] + "..."

    def _categorize_news(self, title: str) -> str:
        """æ ¹æ®æ ‡é¢˜ç¡®å®šæ–°é—»åˆ†ç±»"""
        title_lower = title.lower()

        # èµ„åŠ©å…¬å‘Šç›¸å…³å…³é”®è¯
        if any(keyword in title for keyword in ['ç”³æŠ¥', 'æŒ‡å—', 'ç”³è¯·', 'èµ„åŠ©', 'åŸºé‡‘', 'é¡¹ç›®']):
            return 'funding_announcement'

        # æ”¿ç­–æ›´æ–°
        if any(keyword in title for keyword in ['æ”¿ç­–', 'è§„å®š', 'åŠæ³•', 'é€šçŸ¥', 'ç®¡ç†']):
            return 'policy_update'

        # å¾é›†å…¬å‘Š
        if any(keyword in title for keyword in ['å¾é›†', 'å¾æ±‚', 'æ„è§', 'å»ºè®®']):
            return 'call_for_proposals'

        # ç»“æœå…¬ç¤º
        if any(keyword in title for keyword in ['å…¬ç¤º', 'ç»“æœ', 'åå•', 'è¯„å®¡']):
            return 'results_announcement'

        # é»˜è®¤åˆ†ç±»
        return 'general_announcement'

    def _extract_keywords(self, title: str) -> List[str]:
        """ä»æ ‡é¢˜æå–å…³é”®è¯"""
        keywords = []

        # å¸¸è§çš„ç§‘æŠ€é¡¹ç›®å…³é”®è¯
        keyword_patterns = [
            r'å›½å®¶.*?(?:åŸºé‡‘|é¡¹ç›®|è®¡åˆ’)',
            r'é‡[å¤§ç‚¹].*?(?:é¡¹ç›®|ä¸“é¡¹|å·¥ç¨‹)',
            r'(?:ç”³æŠ¥|ç”³è¯·|å¾é›†).*?(?:æŒ‡å—|é€šçŸ¥)',
            r'(?:ç§‘æŠ€|åˆ›æ–°|ç ”å‘|æŠ€æœ¯)',
            r'(?:èµ„åŠ©|æ”¯æŒ|ç»è´¹)',
        ]

        for pattern in keyword_patterns:
            matches = re.findall(pattern, title)
            keywords.extend(matches)

        # ç§»é™¤é‡å¤å¹¶é™åˆ¶æ•°é‡
        keywords = list(dict.fromkeys(keywords))[:10]

        return keywords

    def _determine_priority(self, title: str, category: str) -> str:
        """ç¡®å®šæ–°é—»ä¼˜å…ˆçº§"""
        title_lower = title.lower()

        # é«˜ä¼˜å…ˆçº§å…³é”®è¯
        high_priority_keywords = ['é‡å¤§', 'é‡ç‚¹', 'å›½å®¶çº§', 'ç´§æ€¥', 'é‡è¦']
        if any(keyword in title for keyword in high_priority_keywords):
            return 'high'

        # èµ„åŠ©å…¬å‘Šé€šå¸¸ä¸ºé«˜ä¼˜å…ˆçº§
        if category == 'funding_announcement':
            return 'high'

        # å¾é›†å…¬å‘Šä¸ºæ™®é€šä¼˜å…ˆçº§
        if category == 'call_for_proposals':
            return 'normal'

        return 'normal'

    def _determine_status(self, title: str, published_date: str) -> str:
        """ç¡®å®šæ–°é—»çŠ¶æ€"""
        try:
            pub_date = datetime.strptime(published_date.split()[0], '%Y-%m-%d')
            days_diff = (datetime.now() - pub_date).days

            # è¶…è¿‡60å¤©çš„æ–°é—»æ ‡è®°ä¸ºè¿‡æœŸ
            if days_diff > 60:
                return 'expired'

            # æ£€æŸ¥æ ‡é¢˜ä¸­æ˜¯å¦æœ‰æˆªæ­¢ç›¸å…³ä¿¡æ¯
            if any(keyword in title for keyword in ['æˆªæ­¢', 'ç»“æŸ', 'å·²ç»“æŸ']):
                return 'expired'

            return 'active'

        except:
            return 'active'

    def _should_include_news(self, news: NewsData, category_filter: Optional[str],
                           date_from: Optional[str], date_to: Optional[str]) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥åŒ…å«è¿™æ¡æ–°é—»"""

        # åˆ†ç±»è¿‡æ»¤
        if category_filter and news.category != category_filter:
            return False

        # æ—¥æœŸè¿‡æ»¤
        if date_from or date_to:
            try:
                news_date = datetime.strptime(news.published_date.split()[0], '%Y-%m-%d')

                if date_from:
                    from_date = datetime.strptime(date_from, '%Y-%m-%d')
                    if news_date < from_date:
                        return False

                if date_to:
                    to_date = datetime.strptime(date_to, '%Y-%m-%d')
                    if news_date > to_date:
                        return False
            except:
                pass  # å¦‚æœæ—¥æœŸè§£æå¤±è´¥ï¼Œä¸è¿›è¡Œè¿‡æ»¤

        return True

    def _make_request_html(self, url: str, **kwargs) -> str:
        """
        å‘é€HTTPè¯·æ±‚å¹¶è¿”å›HTMLå†…å®¹ - ä¼˜åŒ–ç‰ˆrequests
        """
        import requests
        import time

        # åˆ›å»ºsessionä»¥å¤ç”¨è¿æ¥
        session = requests.Session()

        # è®¾ç½®åŸºæœ¬è¯·æ±‚å¤´
        headers = {
            'User-Agent': 'Mozilla/5.0 (Linux; Android 6.0; Nexus 5 Build/MRA58N) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/140.0.0.0 Mobile Safari/537.36 Edg/140.0.0.0',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive'
        }

        print(f"ğŸŒ æ­£åœ¨è¯·æ±‚: {url}")
        start_time = time.time()

        try:
            # ä¼˜åŒ–çš„requestså‚æ•°
            response = session.get(
                url,
                headers=headers,
                timeout=(5, 10),  # è¿æ¥è¶…æ—¶5ç§’ï¼Œè¯»å–è¶…æ—¶10ç§’
                allow_redirects=True,
                stream=False,  # ä¸ä½¿ç”¨æµæ¨¡å¼
                verify=True    # éªŒè¯SSLè¯ä¹¦
            )

            request_time = time.time() - start_time
            print(f"â±ï¸  è¯·æ±‚è€—æ—¶: {request_time:.2f}ç§’")

            # æ£€æŸ¥çŠ¶æ€ç 
            if response.status_code == 200:
                # è®¾ç½®æ­£ç¡®çš„ç¼–ç 
                response.encoding = response.apparent_encoding or 'utf-8'
                return response.text
            else:
                raise Exception(f"HTTPçŠ¶æ€ç é”™è¯¯: {response.status_code}")

        except Exception as e:
            request_time = time.time() - start_time
            print(f"âš ï¸  è¯·æ±‚å¤±è´¥ï¼Œè€—æ—¶: {request_time:.2f}ç§’ï¼Œé”™è¯¯: {e}")
            raise Exception(f"ç½‘ç»œè¯·æ±‚å¤±è´¥: {str(e)}")
        finally:
            session.close()