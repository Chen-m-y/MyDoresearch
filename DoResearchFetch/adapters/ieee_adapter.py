"""
IEEEé€‚é…å™¨
åŸºäºIEEE Xplore API/çˆ¬è™«è·å–æœŸåˆŠè®ºæ–‡æ•°æ®
æ”¯æŒå¼‚æ­¥å¹¶è¡ŒæŠ“å–å®Œæ•´æ‘˜è¦
"""

import re
from typing import Dict, List, Any, Optional
from datetime import datetime, timedelta
from .base import BaseAdapter, PaperData
from utils.exceptions import FetchError, ValidationError
from utils.progress_monitor import create_progress_tracker

# å°è¯•å¯¼å…¥å¼‚æ­¥æ¨¡å—ï¼Œå¦‚æœå¤±è´¥åˆ™æä¾›å›é€€æ–¹æ¡ˆ
try:
    import asyncio
    from utils.async_abstract_fetcher import create_async_service
    ASYNC_AVAILABLE = True
except ImportError:
    ASYNC_AVAILABLE = False
    print("Warning: Async modules not available, falling back to basic abstract extraction")


class IEEEAdapter(BaseAdapter):
    """IEEE Xploreæ•°æ®æºé€‚é…å™¨"""
    
    @property
    def name(self) -> str:
        return "ieee"
    
    @property
    def display_name(self) -> str:
        return "IEEE Xplore"
    
    @property
    def description(self) -> str:
        return "IEEEæœŸåˆŠå’Œä¼šè®®è®ºæ–‡"
    
    @property
    def required_params(self) -> List[str]:
        return ["punumber"]
    
    @property
    def optional_params(self) -> List[str]:
        return ["limit", "early_access", "fetch_full_abstract"]
    
    def _validate_specific_params(self, params: Dict[str, Any]) -> None:
        """IEEEç‰¹å®šçš„å‚æ•°éªŒè¯"""
        punumber = params.get('punumber')
        if not isinstance(punumber, (str, int)):
            raise ValidationError("punumberå¿…é¡»æ˜¯å­—ç¬¦ä¸²æˆ–æ•´æ•°")
        
        limit = params.get('limit', 50)
        if not isinstance(limit, int) or limit <= 0 or limit > 100:
            raise ValidationError("limitå¿…é¡»æ˜¯1-100ä¹‹é—´çš„æ•´æ•°")
    
    def fetch_papers(self, params: Dict[str, Any]) -> Dict[str, Any]:
        """
        æŠ“å–IEEEè®ºæ–‡æ•°æ®
        
        Args:
            params: åŒ…å«punumberå’Œå¯é€‰å‚æ•°çš„å­—å…¸
                - punumber: æœŸåˆŠå‘å¸ƒå·ç 
                - limit: è¿”å›ç»“æœæ•°é‡é™åˆ¶ (é»˜è®¤50, æœ€å¤§100)
                - early_access: æ˜¯å¦è·å–æ—©æœŸè®¿é—®æ–‡ç«  (é»˜è®¤True)
                - fetch_full_abstract: æ˜¯å¦æŠ“å–å®Œæ•´æ‘˜è¦ (é»˜è®¤True)
        """
        punumber = str(params['punumber'])
        limit = params.get('limit', 50)
        early_access = params.get('early_access', True)
        fetch_full_abstract = params.get('fetch_full_abstract', True)  # é»˜è®¤æŠ“å–å®Œæ•´æ‘˜è¦
        
        # åˆ›å»ºè¿›åº¦è¿½è¸ªå™¨
        with create_progress_tracker(f"ieee-{punumber}", limit, f"æŠ“å–IEEEè®ºæ–‡ {punumber}") as tracker:
            try:
                tracker.update(operation="è·å–æœŸåˆŠå…ƒæ•°æ®...")
                
                # 1. è·å–æœŸåˆŠå…ƒæ•°æ®
                metadata = self._fetch_metadata(punumber)
                display_title = metadata.get('displayTitle', f'IEEE Publication {punumber}')
                
                tracker.update(operation="åˆ†ææœŸåˆŠissue...")
                
                # 2. ç¡®å®šä½¿ç”¨å“ªä¸ªissue
                current_issue = metadata.get('currentIssue', {})
                preprint_issue = metadata.get('preprintIssue', {})
                
                # æ ¹æ®early_accesså‚æ•°é€‰æ‹©issue
                target_issue = preprint_issue if early_access and preprint_issue else current_issue
                
                if not target_issue:
                    raise FetchError(
                        f"æ— æ³•æ‰¾åˆ°æœŸåˆŠ {punumber} çš„æœ‰æ•ˆissue",
                        error_code='NO_VALID_ISSUE'
                    )
                
                issue_number = target_issue.get('issueNumber')
                volume = target_issue.get('volume')
                
                if not issue_number:
                    raise FetchError(
                        f"æœŸåˆŠ {punumber} çš„issueä¿¡æ¯ä¸å®Œæ•´",
                        error_code='INCOMPLETE_ISSUE_INFO'
                    )
                
                tracker.update(operation="è·å–è®ºæ–‡ç›®å½•æ•°æ®...")
                
                # 3. è·å–TOCæ•°æ®
                toc_data = self._fetch_toc_data(punumber, issue_number, limit)
                print(toc_data)
                records = toc_data.get('records', [])
                
                # æ›´æ–°æ€»æ•°
                actual_total = len(records)
                tracker.update(operation=f"å¤„ç† {actual_total} ç¯‡è®ºæ–‡...")
                
                # 4. è½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼
                papers = []
                for i, record in enumerate(records):
                    try:
                        paper = self._map_to_paper_data(record, volume, display_title)
                        papers.append(paper)
                        tracker.increment(success=True, operation=f"å¤„ç†è®ºæ–‡ {i+1}/{actual_total}")
                    except Exception as e:
                        # è®°å½•ä½†ä¸ä¸­æ–­å¤„ç†
                        print(f"Warning: Failed to process paper record: {e}")
                        tracker.increment(success=False, operation=f"å¤„ç†è®ºæ–‡å¤±è´¥ {i+1}/{actual_total}")
                        continue
                
                # 5. å¹¶è¡ŒæŠ“å–å®Œæ•´æ‘˜è¦ï¼ˆå¦‚æœå¯ç”¨ä¸”å¯ç”¨ï¼‰
                if fetch_full_abstract and papers:
                    if ASYNC_AVAILABLE:
                        tracker.update(operation=f"å¹¶è¡ŒæŠ“å– {len(papers)} ç¯‡è®ºæ–‡çš„å®Œæ•´æ‘˜è¦...")
                        print(f"ğŸ” Starting parallel abstract fetching for {len(papers)} papers...")
                        try:
                            # è½¬æ¢ä¸ºé€‚åˆå¼‚æ­¥å¤„ç†çš„æ ¼å¼
                            paper_dicts = [paper.to_dict() for paper in papers]
                            
                            # åˆ›å»ºå¼‚æ­¥æœåŠ¡å¹¶æ‰§è¡Œå¹¶è¡ŒæŠ“å–
                            from config import Config
                            async_service = create_async_service(
                                concurrent_limit=Config.MAX_CONCURRENT_REQUESTS,
                                request_delay=Config.ABSTRACT_REQUEST_DELAY
                            )
                            
                            # åœ¨æ–°çš„äº‹ä»¶å¾ªç¯ä¸­è¿è¡Œå¼‚æ­¥ä»»åŠ¡
                            enhanced_papers = asyncio.run(async_service.fetch_abstracts_parallel(paper_dicts))
                            
                            # æ›´æ–°è®ºæ–‡å¯¹è±¡
                            for i, enhanced_dict in enumerate(enhanced_papers):
                                if i < len(papers):
                                    papers[i].abstract = enhanced_dict.get('abstract', papers[i].abstract)
                            
                            print(f"âœ… Parallel abstract fetching completed!")
                            tracker.update(operation="å®Œæ•´æ‘˜è¦æŠ“å–å®Œæˆ")
                            
                        except Exception as e:
                            print(f"âš ï¸  Parallel abstract fetching failed, using original abstracts: {e}")
                            tracker.update(operation="æ‘˜è¦æŠ“å–å¤±è´¥ï¼Œä½¿ç”¨åŸºç¡€æ‘˜è¦")
                    else:
                        print(f"â„¹ï¸  Async not available, using basic abstracts (install aiohttp for parallel fetching)")
                        tracker.update(operation="ä½¿ç”¨åŸºç¡€æ‘˜è¦ï¼ˆæœªå®‰è£…å¹¶è¡ŒæŠ“å–ä¾èµ–ï¼‰")
                
                
                tracker.update(operation="æ’åºå’Œæ•´ç†ç»“æœ...")
                
                # 6. æŒ‰å‘è¡¨æ—¥æœŸå€’åºæ’åºï¼ˆæœ€æ–°çš„åœ¨å‰é¢ï¼‰
                papers.sort(key=lambda x: x.published_date, reverse=True)
                
                # 7. åº”ç”¨limité™åˆ¶
                papers = papers[:limit]
                
                # è½¬æ¢ä¸ºå­—å…¸æ ¼å¼
                paper_dicts = [paper.to_dict() for paper in papers]
                
                # 8. æ„å»ºå“åº”
                total_count = toc_data.get('totalRecords', len(paper_dicts))
                has_more = len(paper_dicts) >= limit and total_count > limit
                
                result = {
                    'papers': paper_dicts,
                    'total_count': total_count,
                    'has_more': has_more,
                    'next_cursor': None,  # IEEE APIé€šå¸¸ä¸æ”¯æŒåˆ†é¡µæ¸¸æ ‡
                    'rate_limit_remaining': None,  # IEEEé€šå¸¸æ²¡æœ‰æ˜ç¡®çš„é™æµä¿¡æ¯
                    'cache_hit': False
                }
                
                # è®¾ç½®ç»“æœåˆ°è¿›åº¦è¿½è¸ªå™¨
                tracker.set_results({
                    'papers_count': len(paper_dicts),
                    'total_available': total_count,
                    'enhanced_abstracts': fetch_full_abstract and ASYNC_AVAILABLE
                })
                
                return result
                
            except FetchError:
                raise
            except Exception as e:
                raise FetchError(
                    f"IEEEæ•°æ®æŠ“å–å¤±è´¥: {str(e)}",
                    error_code='IEEE_FETCH_ERROR'
                )
    
    def _make_request(self, url: str, **kwargs) -> Dict[str, Any]:
        """
        IEEEç‰¹å®šçš„HTTPè¯·æ±‚æ–¹æ³•ï¼ŒåŒ…å«å¿…è¦çš„è¯·æ±‚å¤´
        """
        import requests
        from utils.exceptions import FetchError, RateLimitError
        import time
        
        # è®¾ç½®IEEEç‰¹å®šçš„è¯·æ±‚å¤´
        headers = kwargs.get('headers', {})
        headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'application/json, text/plain, */*',
            'Accept-Language': 'en-US,en;q=0.9',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Referer': 'https://ieeexplore.ieee.org/',
            'Origin': 'https://ieeexplore.ieee.org'
        })
        kwargs['headers'] = headers
        
        # è®¾ç½®é»˜è®¤è¶…æ—¶
        kwargs.setdefault('timeout', self.timeout)
        
        # åˆ›å»ºä¼šè¯ä»¥ä¿æŒcookie
        session = requests.Session()
        
        # é‡è¯•æœºåˆ¶
        for attempt in range(self.max_retries + 1):
            try:
                if 'json' in kwargs:
                    # POSTè¯·æ±‚
                    response = session.post(url, **kwargs)
                else:
                    # GETè¯·æ±‚
                    response = session.get(url, **kwargs)
                
                # æ£€æŸ¥HTTPçŠ¶æ€ç 
                if response.status_code == 429:
                    retry_after = int(response.headers.get('Retry-After', 60))
                    raise RateLimitError(
                        "APIè°ƒç”¨é¢‘ç‡é™åˆ¶",
                        details={'retry_after': retry_after}
                    )
                
                if response.status_code == 418:
                    # ç‰¹æ®Šå¤„ç†418é”™è¯¯ï¼Œå¢åŠ å»¶è¿Ÿåé‡è¯•
                    if attempt < self.max_retries:
                        time.sleep(2 ** attempt + 1)  # æ›´é•¿çš„å»¶è¿Ÿ
                        continue
                    else:
                        raise FetchError(
                            "IEEEæœåŠ¡å™¨æ‹’ç»è¯·æ±‚ï¼Œå¯èƒ½è§¦å‘åçˆ¬è™«æœºåˆ¶",
                            error_code='IEEE_BLOCKED',
                            details={'status_code': response.status_code}
                        )
                
                if response.status_code >= 400:
                    raise FetchError(
                        f"HTTPè¯·æ±‚å¤±è´¥: {response.status_code}",
                        error_code='HTTP_ERROR',
                        details={'status_code': response.status_code, 'response': response.text[:500]}
                    )
                
                return response.json()
                
            except requests.exceptions.Timeout:
                if attempt == self.max_retries:
                    raise FetchError(
                        "è¯·æ±‚è¶…æ—¶",
                        error_code='REQUEST_TIMEOUT'
                    )
                time.sleep(2 ** attempt)  # æŒ‡æ•°é€€é¿
                
            except requests.exceptions.RequestException as e:
                if attempt == self.max_retries:
                    raise FetchError(
                        f"ç½‘ç»œè¯·æ±‚å¤±è´¥: {str(e)}",
                        error_code='NETWORK_ERROR'
                    )
                time.sleep(2 ** attempt)
        
        session.close()
    
    def _fetch_metadata(self, punumber: str) -> Dict[str, Any]:
        """è·å–æœŸåˆŠå…ƒæ•°æ®"""
        url = f"{self.base_url}/rest/publication/home/metadata?pubid={punumber}"
        
        try:
            response = self._make_request(url)
            return response
        except Exception as e:
            raise FetchError(
                f"è·å–æœŸåˆŠå…ƒæ•°æ®å¤±è´¥: {str(e)}",
                error_code='METADATA_FETCH_ERROR'
            )
    
    def _fetch_toc_data(self, punumber: str, issue_number: str, limit: int = 50) -> Dict[str, Any]:
        """è·å–æœŸåˆŠç›®å½•æ•°æ®"""
        url = f"{self.base_url}/rest/search/pub/{punumber}/issue/{issue_number}/toc"
        
        payload = {
            'punumber': punumber,
            'isnumber': issue_number,
            'sortType': "vol-only-newest",
            'rowsPerPage': str(min(limit, 100))  # é™åˆ¶æœ€å¤§100
        }
        
        try:
            response = self._make_request(url, json=payload)
            return response
        except Exception as e:
            raise FetchError(
                f"è·å–TOCæ•°æ®å¤±è´¥: {str(e)}",
                error_code='TOC_FETCH_ERROR'
            )
    
    def _map_to_paper_data(self, record: Dict[str, Any], volume: str, journal_name: str) -> PaperData:
        """å°†IEEEåŸå§‹æ•°æ®æ˜ å°„ä¸ºæ ‡å‡†æ ¼å¼"""
        
        # åŸºæœ¬å­—æ®µ
        title = record.get('articleTitle', '').strip()
        abstract = record.get('abstract', '').strip()
        
        # ä½œè€…ä¿¡æ¯
        authors = []
        if 'authors' in record and isinstance(record['authors'], list):
            authors = [
                author.get('preferredName', author.get('normalizedName', ''))
                for author in record['authors']
                if author.get('preferredName') or author.get('normalizedName')
            ]
        
        # URLå’ŒDOI
        url = record.get('htmlLink', '')
        if url and not url.startswith('http'):
            url = self.base_url + url if url.startswith('/') else f"{self.base_url}/{url}"
        
        doi = record.get('doi', '')
        
        # PDF URL - å°è¯•ä»å¤šä¸ªå¯èƒ½çš„å­—æ®µè·å–
        pdf_url = None
        if 'pdfLink' in record:
            pdf_url = record['pdfLink']
        elif doi:
            # åŸºäºDOIæ„é€ PDF URL
            article_number = record.get('articleNumber', '')
            if article_number:
                pdf_url = f"{self.base_url}/stamp/stamp.jsp?tp=&arnumber={article_number}"
        
        # å‘å¸ƒæ—¥æœŸ
        published_date = self._extract_publication_date(record)
        
        # å…³é”®è¯
        keywords = self._extract_keywords(record)
        
        # å¼•ç”¨æ•°
        citations = record.get('citationCount', 0)
        if isinstance(citations, str) and citations.isdigit():
            citations = int(citations)
        elif not isinstance(citations, int):
            citations = 0
        
        # IEEEç‰¹å®šå­—æ®µ
        source_specific = {
            'ieee_number': record.get('articleNumber', ''),
            'volume': volume,
            'issue': record.get('issue', ''),
            'pages': self._format_pages(record.get('startPage'), record.get('endPage')),
            'publication_number': record.get('punumber', ''),
            'access_type': record.get('accessType', ''),
            'content_type': record.get('contentType', '')
        }
        
        # å…ƒæ•°æ®
        metadata = {
            'publisher': 'IEEE',
            'conference': record.get('publicationTitle'),
            'isbn': record.get('isbn'),
            'issn': record.get('issn'),
            'publication_year': record.get('publicationYear'),
            'is_open_access': record.get('isOpenAccess', False),
            'ieee_terms': record.get('controlledTerms', [])
        }
        
        return PaperData(
            title=title,
            abstract=abstract,
            authors=authors,
            journal=journal_name,
            published_date=published_date,
            url=url,
            pdf_url=pdf_url,
            doi=doi,
            keywords=keywords,
            citations=citations,
            source_specific=source_specific,
            metadata=metadata
        )
    
    def _extract_publication_date(self, record: Dict[str, Any]) -> str:
        """æå–å‘å¸ƒæ—¥æœŸ"""
        # ä¼˜å…ˆä» rightsLink ä¸­æå–æ—¥æœŸ
        rights_link = record.get('rightsLink', '')
        if rights_link and 'publicationDate=' in rights_link:
            import urllib.parse
            try:
                # è§£æURLå‚æ•°
                parsed_url = urllib.parse.urlparse(rights_link)
                query_params = urllib.parse.parse_qs(parsed_url.query)
                pub_date = query_params.get('publicationDate', [''])[0]
                
                if pub_date:
                    # è§£ææ—¥æœŸæ ¼å¼å¦‚: "31+Jul+2025" 
                    pub_date = pub_date.replace('+', ' ')
                    parsed_date = self._parse_date_string(pub_date)
                    if parsed_date:
                        return parsed_date
            except Exception:
                pass  # å¦‚æœè§£æå¤±è´¥ï¼Œç»§ç»­å°è¯•å…¶ä»–æ–¹æ³•
        
        # å°è¯•å¤šä¸ªæ—¥æœŸå­—æ®µ
        date_fields = ['publicationDate', 'insertDate', 'publishedDate', 'onlineDate', 'coverDate']
        
        for field in date_fields:
            if field in record and record[field]:
                date_str = record[field]
                parsed_date = self._parse_date(date_str)
                if parsed_date:
                    return parsed_date
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°å…·ä½“æ—¥æœŸï¼Œå°è¯•ä½¿ç”¨å¹´ä»½
        year = record.get('publicationYear')
        if year:
            try:
                year_int = int(year)
                return f"{year_int}-01-01"
            except (ValueError, TypeError):
                pass
        
        # é»˜è®¤è¿”å›å½“å‰æ—¥æœŸ
        return datetime.now().strftime('%Y-%m-%d')
    
    def _parse_date_string(self, date_str: str) -> Optional[str]:
        """è§£æç‰¹æ®Šæ ¼å¼çš„æ—¥æœŸå­—ç¬¦ä¸²ï¼Œå¦‚ '31 Jul 2025'"""
        if not date_str:
            return None
            
        try:
            from datetime import datetime
            
            # æ¸…ç†å­—ç¬¦ä¸²
            date_str = date_str.strip()
            
            # å°è¯•è§£æ "DD MMM YYYY" æ ¼å¼ï¼Œå¦‚ "31 Jul 2025"
            try:
                date_obj = datetime.strptime(date_str, '%d %b %Y')
                return date_obj.strftime('%Y-%m-%d')
            except ValueError:
                pass
            
            # å°è¯•è§£æ "DD Month YYYY" æ ¼å¼ï¼Œå¦‚ "31 July 2025"
            try:
                date_obj = datetime.strptime(date_str, '%d %B %Y')
                return date_obj.strftime('%Y-%m-%d')
            except ValueError:
                pass
                
            # å°è¯•è§£æ "MMM DD YYYY" æ ¼å¼ï¼Œå¦‚ "Jul 31 2025"
            try:
                date_obj = datetime.strptime(date_str, '%b %d %Y')
                return date_obj.strftime('%Y-%m-%d')
            except ValueError:
                pass
                
            # å°è¯•è§£æ "Month DD YYYY" æ ¼å¼ï¼Œå¦‚ "July 31 2025"
            try:
                date_obj = datetime.strptime(date_str, '%B %d %Y')
                return date_obj.strftime('%Y-%m-%d')
            except ValueError:
                pass
                
        except Exception:
            pass
            
        # å¦‚æœç‰¹æ®Šæ ¼å¼éƒ½å¤±è´¥äº†ï¼Œå°è¯•é€šç”¨è§£æ
        return self._parse_date(date_str)
    
    def _parse_date(self, date_str: str) -> Optional[str]:
        """è§£æå„ç§æ ¼å¼çš„æ—¥æœŸå­—ç¬¦ä¸²"""
        if not date_str:
            return None
        
        # å¸¸è§çš„æ—¥æœŸæ ¼å¼
        date_patterns = [
            r'(\d{4})-(\d{2})-(\d{2})',  # YYYY-MM-DD
            r'(\d{4})/(\d{2})/(\d{2})',  # YYYY/MM/DD
            r'(\d{2})-(\d{2})-(\d{4})',  # DD-MM-YYYY
            r'(\d{2})/(\d{2})/(\d{4})',  # DD/MM/YYYY
            r'(\d{4})',  # ä»…å¹´ä»½
        ]
        
        for pattern in date_patterns:
            match = re.search(pattern, str(date_str))
            if match:
                groups = match.groups()
                if len(groups) == 1:  # ä»…å¹´ä»½
                    return f"{groups[0]}-01-01"
                elif len(groups) == 3:
                    # åˆ¤æ–­æ˜¯å¦ä¸ºYYYY-MM-DDæ ¼å¼
                    if len(groups[0]) == 4:
                        year, month, day = groups
                    else:  # DD-MM-YYYYæ ¼å¼
                        day, month, year = groups
                    
                    try:
                        # éªŒè¯æ—¥æœŸæœ‰æ•ˆæ€§
                        datetime(int(year), int(month), int(day))
                        return f"{year}-{month.zfill(2)}-{day.zfill(2)}"
                    except ValueError:
                        continue
        
        return None
    
    def _extract_keywords(self, record: Dict[str, Any]) -> List[str]:
        """æå–å…³é”®è¯"""
        keywords = []
        
        # IEEEç‰¹å®šçš„æœ¯è¯­å­—æ®µ
        if 'indexTerms' in record:
            terms = record['indexTerms']
            if isinstance(terms, dict):
                for category, terms_list in terms.items():
                    if isinstance(terms_list, list):
                        keywords.extend([term.get('term', '') for term in terms_list if isinstance(term, dict)])
                    elif isinstance(terms_list, str):
                        keywords.append(terms_list)
        
        # å—æ§è¯æ±‡
        if 'controlledTerms' in record and isinstance(record['controlledTerms'], list):
            keywords.extend(record['controlledTerms'])
        
        # ä½œè€…å…³é”®è¯
        if 'authorTerms' in record and isinstance(record['authorTerms'], list):
            keywords.extend(record['authorTerms'])
        
        # æ¸…ç†å¹¶å»é‡
        keywords = [kw.strip() for kw in keywords if kw and isinstance(kw, str) and kw.strip()]
        return list(dict.fromkeys(keywords))  # ä¿æŒé¡ºåºçš„å»é‡
    
    def _format_pages(self, start_page: Any, end_page: Any) -> str:
        """æ ¼å¼åŒ–é¡µç èŒƒå›´"""
        if start_page and end_page:
            return f"{start_page}-{end_page}"
        elif start_page:
            return str(start_page)
        elif end_page:
            return str(end_page)
        else:
            return ""