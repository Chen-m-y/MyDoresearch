"""
å¼‚æ­¥å¹¶è¡Œæ‘˜è¦æŠ“å–æœåŠ¡
åŸºäº asyncio + aiohttp å®ç°é«˜æ•ˆçš„IEEEæ‘˜è¦è·å–
"""

import asyncio
import aiohttp
import json
import re
import time
import logging
from typing import List, Dict, Optional, Tuple
from dataclasses import dataclass


@dataclass
class FetchMetrics:
    """æŠ“å–æ€§èƒ½æŒ‡æ ‡"""
    total_papers: int
    successful_abstracts: int
    cache_hits: int
    total_time: float
    avg_response_time: float
    error_count: int


class IEEEAsyncService:
    """IEEEå¼‚æ­¥æ‘˜è¦æŠ“å–æœåŠ¡"""
    
    def __init__(self, concurrent_limit: int = 8, request_delay: float = 0.1):
        self.concurrent_limit = concurrent_limit
        self.request_delay = request_delay
        self.semaphore = asyncio.Semaphore(concurrent_limit)
        self.session_timeout = aiohttp.ClientTimeout(total=15)
        
        # æ€§èƒ½ç›‘æ§
        self.metrics = FetchMetrics(0, 0, 0, 0.0, 0.0, 0)
        
        # HTTPå¤´è®¾ç½®
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.9',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Referer': 'https://ieeexplore.ieee.org/'
        }

    async def fetch_abstracts_parallel(self, papers: List[Dict]) -> List[Dict]:
        """
        å¹¶è¡Œè·å–è®ºæ–‡å®Œæ•´æ‘˜è¦ï¼ˆæ”¯æŒç¼“å­˜ï¼‰
        
        Args:
            papers: åŒ…å«åŸºæœ¬ä¿¡æ¯çš„è®ºæ–‡åˆ—è¡¨
            
        Returns:
            åŒ…å«å®Œæ•´æ‘˜è¦çš„è®ºæ–‡åˆ—è¡¨
        """
        if not papers:
            return []
        
        # å¯¼å…¥ç¼“å­˜åŠŸèƒ½
        from utils.abstract_cache import get_cached_abstract, cache_abstract
        
        start_time = time.time()
        self.metrics = FetchMetrics(len(papers), 0, 0, 0.0, 0.0, 0)
        
        print(f"ğŸš€ Starting parallel abstract fetching for {len(papers)} papers...")
        
        # 1. æ£€æŸ¥ç¼“å­˜ï¼Œåˆ†ç¦»éœ€è¦æŠ“å–çš„è®ºæ–‡
        papers_to_fetch = []
        papers_with_cache = []
        cache_hits = 0
        
        for i, paper in enumerate(papers):
            article_number = self._extract_article_number(paper)
            if article_number:
                cached_abstract = get_cached_abstract(article_number, "ieee")
                if cached_abstract:
                    # ä½¿ç”¨ç¼“å­˜çš„æ‘˜è¦
                    paper['abstract'] = cached_abstract
                    papers_with_cache.append(paper)
                    cache_hits += 1
                    print(f"ğŸ“‹ Paper {i+1}: Using cached abstract ({len(cached_abstract)} chars)")
                else:
                    # éœ€è¦æŠ“å–
                    papers_to_fetch.append((i, paper, article_number))
            else:
                # æ— æ³•æå–æ–‡ç« ç¼–å·ï¼Œä¿æŒåŸæ ·
                papers_with_cache.append(paper)
        
        self.metrics.cache_hits = cache_hits
        print(f"ğŸ“Š Cache status: {cache_hits} hits, {len(papers_to_fetch)} need fetching")
        
        # 2. å¦‚æœæ‰€æœ‰æ‘˜è¦éƒ½å·²ç¼“å­˜ï¼Œç›´æ¥è¿”å›
        if not papers_to_fetch:
            self.metrics.total_time = time.time() - start_time
            print(f"âœ… All abstracts available from cache!")
            self._log_performance_report()
            return papers_with_cache
        
        # 3. å¹¶è¡ŒæŠ“å–æœªç¼“å­˜çš„æ‘˜è¦
        print(f"ğŸ” Fetching {len(papers_to_fetch)} uncached abstracts...")
        
        # åˆ›å»ºè¿æ¥å™¨ï¼Œé™åˆ¶è¿æ¥æ•°
        connector = aiohttp.TCPConnector(
            limit=50,
            limit_per_host=self.concurrent_limit,
            ttl_dns_cache=300,
            use_dns_cache=True,
        )
        
        async with aiohttp.ClientSession(
            connector=connector,
            timeout=self.session_timeout,
            headers=self.headers
        ) as session:
            
            # åˆ›å»ºå¹¶å‘ä»»åŠ¡ï¼ˆåªé’ˆå¯¹æœªç¼“å­˜çš„è®ºæ–‡ï¼‰
            tasks = [
                self._fetch_single_abstract_with_cache(session, paper, article_number, original_index)
                for original_index, paper, article_number in papers_to_fetch
            ]
            
            # æ‰§è¡Œæ‰€æœ‰ä»»åŠ¡ï¼Œä½¿ç”¨gatheræ”¶é›†ç»“æœ
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            # å¤„ç†ç»“æœå’Œå¼‚å¸¸
            processed_results = []
            for i, result in enumerate(results):
                original_index, paper, article_number = papers_to_fetch[i]
                
                if isinstance(result, Exception):
                    logging.error(f"Task for paper {original_index+1} failed: {result}")
                    self.metrics.error_count += 1
                    processed_results.append((original_index, paper))
                else:
                    processed_results.append((original_index, result))
        
        # 4. åˆå¹¶ç¼“å­˜çš„å’Œæ–°æŠ“å–çš„ç»“æœ
        final_results = [None] * len(papers)
        
        # æ”¾ç½®ç¼“å­˜çš„ç»“æœ
        cache_index = 0
        for i, paper in enumerate(papers):
            article_number = self._extract_article_number(paper)
            if not article_number or get_cached_abstract(article_number, "ieee"):
                if cache_index < len(papers_with_cache):
                    final_results[i] = papers_with_cache[cache_index]
                    cache_index += 1
                else:
                    final_results[i] = paper
        
        # æ”¾ç½®æ–°æŠ“å–çš„ç»“æœ
        for original_index, paper in processed_results:
            final_results[original_index] = paper
        
        # è¿‡æ»¤Noneå€¼
        final_results = [paper for paper in final_results if paper is not None]
        
        # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
        self.metrics.total_time = time.time() - start_time
        self.metrics.avg_response_time = self.metrics.total_time / len(papers_to_fetch) if papers_to_fetch else 0
        
        # è¾“å‡ºæ€§èƒ½æŠ¥å‘Š
        self._log_performance_report()
        
        return final_results

    async def _fetch_single_abstract_with_cache(self, session: aiohttp.ClientSession, paper: Dict, article_number: str, index: int) -> Dict:
        """
        è·å–å•ç¯‡è®ºæ–‡çš„å®Œæ•´æ‘˜è¦ï¼ˆæ”¯æŒç¼“å­˜ï¼‰
        
        Args:
            session: aiohttpä¼šè¯
            paper: è®ºæ–‡ä¿¡æ¯
            article_number: æ–‡ç« ç¼–å·
            index: è®ºæ–‡ç´¢å¼•ï¼ˆç”¨äºæ—¥å¿—ï¼‰
            
        Returns:
            åŒ…å«å®Œæ•´æ‘˜è¦çš„è®ºæ–‡ä¿¡æ¯
        """
        async with self.semaphore:  # æ§åˆ¶å¹¶å‘æ•°
            try:
                # å¯¼å…¥ç¼“å­˜åŠŸèƒ½
                from utils.abstract_cache import cache_abstract
                
                # æ„å»ºè¯¦ç»†é¡µé¢URL
                detail_url = f"https://ieeexplore.ieee.org/document/{article_number}"
                
                # æ·»åŠ å»¶è¿Ÿé¿å…è¢«é™æµ
                await asyncio.sleep(self.request_delay)
                
                # å‘é€HTTPè¯·æ±‚
                async with session.get(detail_url) as response:
                    if response.status != 200:
                        logging.warning(f"HTTP {response.status} for paper {index+1}: {article_number}")
                        return paper
                    
                    html_content = await response.text()
                    
                    # è§£æé¡µé¢è·å–å®Œæ•´æ‘˜è¦
                    full_abstract = self._extract_abstract_from_html(html_content)
                    
                    if full_abstract and len(full_abstract) > len(paper.get('abstract', '')):
                        # æ›´æ–°è®ºæ–‡æ‘˜è¦
                        paper['abstract'] = full_abstract
                        self.metrics.successful_abstracts += 1
                        
                        # ç¼“å­˜æ–°çš„å®Œæ•´æ‘˜è¦
                        cache_abstract(
                            article_id=article_number,
                            url=detail_url,
                            title=paper.get('title', ''),
                            abstract=full_abstract,
                            source='ieee'
                        )
                        
                        print(f"âœ“ Paper {index+1}: Updated and cached abstract ({len(full_abstract)} chars)")
                    else:
                        print(f"âš  Paper {index+1}: No improvement found")
                    
                    return paper
                    
            except asyncio.TimeoutError:
                logging.error(f"Timeout for paper {index+1}")
                self.metrics.error_count += 1
                return paper
                
            except Exception as e:
                logging.error(f"Error fetching abstract for paper {index+1}: {e}")
                self.metrics.error_count += 1
                return paper

    async def _fetch_single_abstract(self, session: aiohttp.ClientSession, paper: Dict, index: int) -> Dict:
        """
        è·å–å•ç¯‡è®ºæ–‡çš„å®Œæ•´æ‘˜è¦
        
        Args:
            session: aiohttpä¼šè¯
            paper: è®ºæ–‡ä¿¡æ¯
            index: è®ºæ–‡ç´¢å¼•ï¼ˆç”¨äºæ—¥å¿—ï¼‰
            
        Returns:
            åŒ…å«å®Œæ•´æ‘˜è¦çš„è®ºæ–‡ä¿¡æ¯
        """
        async with self.semaphore:  # æ§åˆ¶å¹¶å‘æ•°
            try:
                # ä»paperä¸­è·å–æ–‡ç« ç¼–å·
                article_number = self._extract_article_number(paper)
                if not article_number:
                    return paper
                
                # æ„å»ºè¯¦ç»†é¡µé¢URL
                detail_url = f"https://ieeexplore.ieee.org/document/{article_number}"
                
                # æ·»åŠ å»¶è¿Ÿé¿å…è¢«é™æµ
                await asyncio.sleep(self.request_delay)
                
                # å‘é€HTTPè¯·æ±‚
                async with session.get(detail_url) as response:
                    if response.status != 200:
                        logging.warning(f"HTTP {response.status} for paper {index+1}: {article_number}")
                        return paper
                    
                    html_content = await response.text()
                    
                    # è§£æé¡µé¢è·å–å®Œæ•´æ‘˜è¦
                    full_abstract = self._extract_abstract_from_html(html_content)
                    
                    if full_abstract and len(full_abstract) > len(paper.get('abstract', '')):
                        paper['abstract'] = full_abstract
                        self.metrics.successful_abstracts += 1
                        
                        print(f"âœ“ Paper {index+1}: Updated abstract ({len(full_abstract)} chars)")
                    else:
                        print(f"âš  Paper {index+1}: No improvement found")
                    
                    return paper
                    
            except asyncio.TimeoutError:
                logging.error(f"Timeout for paper {index+1}")
                self.metrics.error_count += 1
                return paper
                
            except Exception as e:
                logging.error(f"Error fetching abstract for paper {index+1}: {e}")
                self.metrics.error_count += 1
                return paper

    def _extract_article_number(self, paper: Dict) -> Optional[str]:
        """ä»è®ºæ–‡ä¿¡æ¯ä¸­æå–æ–‡ç« ç¼–å·"""
        # è°ƒè¯•ï¼šæ‰“å°paperç»“æ„
        print(f"Debug: Extracting article number from paper keys: {list(paper.keys())}")
        
        # 1. é¦–å…ˆæ£€æŸ¥ source_specific å­—æ®µä¸­çš„ ieee_number
        if 'source_specific' in paper and isinstance(paper['source_specific'], dict):
            ieee_number = paper['source_specific'].get('ieee_number')
            if ieee_number:
                article_num = str(ieee_number).strip()
                if article_num.isdigit():
                    print(f"Debug: Found IEEE number in source_specific: {article_num}")
                    return article_num
        
        # 2. å°è¯•ä»URLä¸­æå–
        url_fields = ['url', 'documentLink', 'htmlLink']
        for field in url_fields:
            if field in paper and paper[field]:
                value = str(paper[field])
                print(f"Debug: Checking {field} = {value}")
                
                # ä» /document/12345678/ æˆ– https://ieeexplore.ieee.org/document/12345678/ æå–
                match = re.search(r'/document/(\d+)', value)
                if match:
                    article_num = match.group(1)
                    print(f"Debug: Extracted article number from URL: {article_num}")
                    return article_num
        
        # 3. å°è¯•å…¶ä»–å¯èƒ½çš„å­—æ®µ
        other_fields = ['articleNumber', 'ieee_number', 'article_number']
        for field in other_fields:
            if field in paper and paper[field]:
                value = str(paper[field]).strip()
                print(f"Debug: Found {field} = {value}")
                
                if value.isdigit():
                    print(f"Debug: Direct article number: {value}")
                    return value
        
        print("Debug: No article number found!")
        return None

    def _extract_abstract_from_html(self, html_content: str) -> Optional[str]:
        """
        ä»IEEEé¡µé¢HTMLä¸­æå–å®Œæ•´æ‘˜è¦
        åŸºäºæä¾›çš„å‚è€ƒä»£ç ï¼Œä½¿ç”¨æ­£ç¡®çš„æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…xplGlobal.document.metadata
        """
        try:
            # ä¸»è¦æ–¹æ³•ï¼šä½¿ç”¨ä¸å‚è€ƒä»£ç ç›¸åŒçš„æ­£åˆ™è¡¨è¾¾å¼
            matches = re.search(r'xplGlobal\.document\.metadata=(\{.*?\});', html_content, re.DOTALL)
            
            if matches:
                try:
                    # è·å–JSONå­—ç¬¦ä¸²
                    json_str = matches.group(1)
                    # å°†JSONå­—ç¬¦ä¸²è½¬æ¢ä¸ºå­—å…¸
                    metadata = json.loads(json_str)
                    abstract = metadata.get('abstract', '')
                    
                    if abstract and len(abstract) > 50:  # ç¡®ä¿æ˜¯æœ‰æ•ˆçš„æ‘˜è¦
                        print(f"âœ“ Successfully extracted abstract ({len(abstract)} chars)")
                        return abstract.strip()
                    else:
                        print(f"âš  Abstract too short or empty: {len(abstract)} chars")
                        
                except json.JSONDecodeError as e:
                    print(f"âœ— JSON decode error: {e}")
                    logging.debug(f"Failed to parse JSON: {json_str[:200]}...")
            else:
                print("âœ— xplGlobal.document.metadata not found in HTML")
            
            # å¤‡ç”¨æ–¹æ³•1: å°è¯•å…¶ä»–å¯èƒ½çš„metadataå˜é‡
            backup_patterns = [
                r'document\.metadata\s*=\s*(\{.*?\});',
                r'metadata\s*=\s*(\{.*?\});'
            ]
            
            for pattern in backup_patterns:
                match = re.search(pattern, html_content, re.DOTALL)
                if match:
                    try:
                        json_str = match.group(1)
                        metadata = json.loads(json_str)
                        abstract = metadata.get('abstract', '')
                        if abstract and len(abstract) > 50:
                            print(f"âœ“ Extracted from backup pattern ({len(abstract)} chars)")
                            return abstract.strip()
                    except json.JSONDecodeError:
                        continue
            
            # å¤‡ç”¨æ–¹æ³•2: ç›´æ¥ä»HTMLå…ƒç´ ä¸­æå–
            try:
                from bs4 import BeautifulSoup
                soup = BeautifulSoup(html_content, 'html.parser')
                
                abstract_selectors = [
                    '.abstract-text',
                    '.abstract .description', 
                    '[data-testid="abstract-text"]',
                    '.u-mb-1.stats-abstract-text',
                    '.abstract-desktop-div .u-mb-1'
                ]
                
                for selector in abstract_selectors:
                    element = soup.select_one(selector)
                    if element:
                        text = element.get_text(strip=True)
                        if len(text) > 50:
                            print(f"âœ“ Extracted from HTML element ({len(text)} chars)")
                            return text
            except ImportError:
                print("BeautifulSoup not available for HTML parsing")
            
            print("âœ— No abstract found using any method")
            return None
            
        except Exception as e:
            print(f"âœ— Error extracting abstract: {e}")
            logging.error(f"Error extracting abstract: {e}")
            return None

    def _decode_html_entities(self, text: str) -> str:
        """è§£ç HTMLå®ä½“"""
        import html
        return html.unescape(text)

    def _log_performance_report(self):
        """è¾“å‡ºæ€§èƒ½æŠ¥å‘Šï¼ˆåŒ…å«ç¼“å­˜ç»Ÿè®¡ï¼‰"""
        success_rate = (self.metrics.successful_abstracts / self.metrics.total_papers * 100) if self.metrics.total_papers > 0 else 0
        error_rate = (self.metrics.error_count / self.metrics.total_papers * 100) if self.metrics.total_papers > 0 else 0
        cache_rate = (self.metrics.cache_hits / self.metrics.total_papers * 100) if self.metrics.total_papers > 0 else 0
        
        print(f"""
ğŸ“Š å¹¶è¡ŒæŠ“å–æ€§èƒ½æŠ¥å‘Š:
   æ€»è®ºæ–‡æ•°: {self.metrics.total_papers}
   ç¼“å­˜å‘½ä¸­: {self.metrics.cache_hits} ({cache_rate:.1f}%)
   æˆåŠŸè·å–: {self.metrics.successful_abstracts} ({success_rate:.1f}%)
   é”™è¯¯æ•°é‡: {self.metrics.error_count} ({error_rate:.1f}%)
   æ€»è€—æ—¶: {self.metrics.total_time:.2f}ç§’
   å¹³å‡è€—æ—¶: {self.metrics.avg_response_time:.2f}ç§’/ç¯‡
   å¹¶å‘æ•°: {self.concurrent_limit}
        """)
        
        # æ€§èƒ½å»ºè®®
        if self.metrics.avg_response_time > 3.0:
            print("ğŸ’¡ å»ºè®®: å¹³å‡å“åº”æ—¶é—´è¾ƒæ…¢ï¼Œå¯ä»¥å‡å°‘å¹¶å‘æ•°")
        elif self.metrics.avg_response_time < 0.5:
            print("ğŸ’¡ å»ºè®®: å“åº”æ—¶é—´å¾ˆå¿«ï¼Œå¯ä»¥é€‚å½“å¢åŠ å¹¶å‘æ•°")
        
        if error_rate > 20:
            print("âš ï¸  è­¦å‘Š: é”™è¯¯ç‡è¾ƒé«˜ï¼Œå¯èƒ½è§¦å‘äº†åçˆ¬è™«æœºåˆ¶")
        
        if cache_rate > 50:
            print("ğŸ‰ ç¼“å­˜æ•ˆæœè‰¯å¥½ï¼Œå¤§é‡é‡å¤è¯·æ±‚è¢«é¿å…")
        elif cache_rate < 10:
            print("ğŸ“‹ ç¼“å­˜å‘½ä¸­ç‡è¾ƒä½ï¼Œè¿™æ˜¯æ­£å¸¸çš„é¦–æ¬¡æŠ“å–")


# å·¥å‚å‡½æ•°
def create_async_service(concurrent_limit: int = 8, request_delay: float = 0.1) -> IEEEAsyncService:
    """åˆ›å»ºå¼‚æ­¥æœåŠ¡å®ä¾‹"""
    return IEEEAsyncService(concurrent_limit=concurrent_limit, request_delay=request_delay)